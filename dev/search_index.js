var documenterSearchIndex = {"docs":
[{"location":"#[ParallelKMeans.jl-Package](https://github.com/PyDataBlog/ParallelKMeans.jl)","page":"Home","title":"ParallelKMeans.jl Package","text":"","category":"section"},{"location":"#Motivation","page":"Home","title":"Motivation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"It's actually a funny story led to the development of this package. What started off as a personal toy project trying to re-construct the K-Means algorithm in native Julia blew up after a heated discussion on the Julia Discourse forum when I asked for Julia optimization tips. Long story short, Julia community is an amazing one! Andrey offered his help and together, we decided to push the speed limits of Julia with a parallel implementation of the most famous clustering algorithm. The initial results were mind blowing so we have decided to tidy up the implementation and share with the world as a maintained Julia pacakge.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Say hello to ParallelKMeans!","category":"page"},{"location":"","page":"Home","title":"Home","text":"This package aims to utilize the speed of Julia and parallelization (both CPU & GPU) to offer an extremely fast implementation of the K-Means clustering algorithm and its variants via a friendly interface for practioners.","category":"page"},{"location":"","page":"Home","title":"Home","text":"In short, we hope this package will eventually mature as the \"one-stop-shop\" for everything K-Means on CPUs and GPUs.","category":"page"},{"location":"#K-Means-Algorithm-Implementation-Notes","page":"Home","title":"K-Means Algorithm Implementation Notes","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Since Julia is a column major language, the input (design matrix) expected by the package must be in the following format;","category":"page"},{"location":"","page":"Home","title":"Home","text":"Design matrix X of size nÃ—m, the i-th column of X (X[:, i]) is a single data point in n-dimensional space.\nThus, the rows of the design matrix represent the feature space with the columns representing all the training samples in this feature space.","category":"page"},{"location":"","page":"Home","title":"Home","text":"One of the pitfalls of K-Means algorithm is that it can fall into a local minima. This implementation inherits this problem like every implementation does. As a result, it is useful in practice to restart it several times to get the correct results.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you are using  Julia in the recommended Juno IDE, the number of threads is already set to the number of available CPU cores so multithreading enabled out of the box. For other IDEs, multithreading must be exported in your environment before launching the Julia REPL in the command line.","category":"page"},{"location":"","page":"Home","title":"Home","text":"TIP: One needs to navigate or point to the Julia executable file to be able to launch it in the command line. Enable multi threading on Mac/Linux systems via;","category":"page"},{"location":"","page":"Home","title":"Home","text":"export JULIA_NUM_THREADS=n  # where n is the number of threads/cores","category":"page"},{"location":"","page":"Home","title":"Home","text":"For Windows systems:","category":"page"},{"location":"","page":"Home","title":"Home","text":"set JULIA_NUM_THREADS=n  # where n is the number of threads/cores","category":"page"},{"location":"","page":"Home","title":"Home","text":"You can grab the latest stable version of this package from Julia registries by simply running;","category":"page"},{"location":"","page":"Home","title":"Home","text":"NB: Don't forget to invoke Julia's package manager with ]","category":"page"},{"location":"","page":"Home","title":"Home","text":"pkg> add ParallelKMeans","category":"page"},{"location":"","page":"Home","title":"Home","text":"The few (and selected) brave ones can simply grab the current experimental features by simply adding the master branch to your development environment after invoking the package manager with ]:","category":"page"},{"location":"","page":"Home","title":"Home","text":"pkg> add ParallelKMeans#master","category":"page"},{"location":"","page":"Home","title":"Home","text":"You are good to go with bleeding edge features and breakages!","category":"page"},{"location":"","page":"Home","title":"Home","text":"To revert to a stable version, you can simply run:","category":"page"},{"location":"","page":"Home","title":"Home","text":"pkg> free ParallelKMeans","category":"page"},{"location":"#Features","page":"Home","title":"Features","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Lightning fast implementation of Kmeans clustering algorithm even on a single thread in native Julia.\nSupport for multi-threading implementation of K-Means clustering algorithm.\n'Kmeans++' initialization for faster and better convergence.\nImplementation of available classic and contemporary variants of the K-Means algorithm.","category":"page"},{"location":"#Pending-Features","page":"Home","title":"Pending Features","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"[X] Implementation of Hamerly implementation.\n[X] Interface for inclusion in Alan Turing Institute's MLJModels.\n[X] Full Implementation of Triangle inequality based on Elkan - 2003 Using the Triangle Inequality to Accelerate K-Means\".\n[X] Implementation of Yinyang K-Means: A Drop-In Replacement of the Classic K-Means with Consistent Speedup.\n[X] Implementation of Coresets.\n[X] Support for weighted K-means.\n[X] Support of MLJ Random generation hyperparameter.\n[X] Implementation of Mini-batch KMeans variant\n[ ] Support for other distance metrics supported by Distances.jl.\n[ ] Implementation of Geometric methods to accelerate k-means algorithm.\n[ ] Native support for tabular data inputs outside of MLJModels' interface.\n[ ] GPU support?\n[ ] Distributed calculations support.\n[ ] Optimization of code base.\n[ ] Improved Documentation\n[ ] More benchmark tests.","category":"page"},{"location":"#How-To-Use","page":"Home","title":"How To Use","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Taking advantage of Julia's brilliant multiple dispatch system, the package exposes users to a very easy-to-use API.","category":"page"},{"location":"","page":"Home","title":"Home","text":"using ParallelKMeans\n\n# Uses all available CPU cores by default\nmulti_results = kmeans(X, 3; max_iters=300)\n\n# Use only 1 core of CPU\nresults = kmeans(X, 3; n_threads=1, max_iters=300)","category":"page"},{"location":"","page":"Home","title":"Home","text":"The main design goal is to offer all available variations of the KMeans algorithm to end users as composable elements. By default, Lloyd's implementation is used but users can specify different variations of the KMeans clustering algorithm via this interface;","category":"page"},{"location":"","page":"Home","title":"Home","text":"some_results = kmeans([algo], input_matrix, k; kwargs)\n\n# example\nr = kmeans(Lloyd(), X, 3)  # same result as the default","category":"page"},{"location":"","page":"Home","title":"Home","text":"# r contains all the learned artifacts that can be accessed as;\nr.centers               # cluster centers (d x k)\nr.assignments           # label assignments (n)\nr.totalcost             # total cost (i.e. objective)\nr.iterations            # number of elapsed iterations\nr.converged             # whether the procedure converged","category":"page"},{"location":"#Supported-KMeans-algorithm-variations-and-recommended-use-cases","page":"Home","title":"Supported KMeans algorithm variations and recommended use cases","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Lloyd()  - Default algorithm but only recommended for very small matrices (switch to n_threads = 1 to avoid overhead).\nHamerly() - Hamerly is good for moderate number of clusters (< 50?) and moderate dimensions (<100?).\nElkan() - Recommended for high dimensional data.\nYinyang() - Recommended for large dimensions and/or large number of clusters.\nCoreset() - Recommended for very fast clustering of very large datasets, when extreme accuracy is not important.\nMiniBatch() - Recommended for extremely large datasets, when extreme accuracy is not important.\nGeometric() - (Coming soon)","category":"page"},{"location":"#Practical-Usage-Examples","page":"Home","title":"Practical Usage Examples","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Some of the common usage examples of this package are as follows:","category":"page"},{"location":"#Clustering-With-A-Desired-Number-Of-Groups","page":"Home","title":"Clustering With A Desired Number Of Groups","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using ParallelKMeans, RDatasets, Plots\n\n# load the data\niris = dataset(\"datasets\", \"iris\");\n\n# features to use for clustering\nfeatures = collect(Matrix(iris[:, 1:4])');\n\n# various artifacts can be accessed from the result i.e. assigned labels, cost value etc\nresult = kmeans(features, 3);\n\n# plot with the point color mapped to the assigned cluster index\nscatter(iris.PetalLength, iris.PetalWidth, marker_z=result.assignments,\n        color=:lightrainbow, legend=false)\n","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: Image description)","category":"page"},{"location":"#Elbow-Method-For-The-Selection-Of-optimal-number-of-clusters","page":"Home","title":"Elbow Method For The Selection Of optimal number of clusters","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using ParallelKMeans\n\n# Single Thread Implementation of Lloyd's Algorithm\nb = [ParallelKMeans.kmeans(X, i, n_threads=1; tol=1e-6, max_iters=300, verbose=false).totalcost for i = 2:10]\n\n# Multi-threaded Implementation of Lloyd's Algorithm by default\nc = [ParallelKMeans.kmeans(X, i; tol=1e-6, max_iters=300, verbose=false).totalcost for i = 2:10]\n","category":"page"},{"location":"#Benchmarks","page":"Home","title":"Benchmarks","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Currently, this package is benchmarked against similar implementations in both Python and Julia. All reproducible benchmarks can be found in ParallelKMeans/extras directory. More tests in various languages are planned beyond the initial release version (0.1.0).","category":"page"},{"location":"","page":"Home","title":"Home","text":"Note: All benchmark tests are made on the same computer to help eliminate any bias.","category":"page"},{"location":"","page":"Home","title":"Home","text":"PC Name CPU Ram\niMac (Retina 5K 27-inch 2019) 3 GHz 6-Core Intel Core i5 24 GB 2667 MHz DDR4","category":"page"},{"location":"","page":"Home","title":"Home","text":"Currently, the benchmark speed tests are based on the search for optimal number of clusters using the Elbow Method since this is a practical use case for most practioners employing the K-Means algorithm.","category":"page"},{"location":"#Benchmark-Results","page":"Home","title":"Benchmark Results","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"(Image: benchmark_image.png)","category":"page"},{"location":"","page":"Home","title":"Home","text":"_________________________________________________________________________________________________________","category":"page"},{"location":"","page":"Home","title":"Home","text":"1 million sample (secs) 100k sample (secs) 10k sample (secs) 1k sample (secs) package language\n538.53100 33.15700 0.74238 0.01710 Clustering.jl Julia\n220.35700 20.93600 0.82430 0.02639 mlpack C++ Wrapper\n20.55400 2.91300 0.17559 0.00609 Lloyd Julia\n11.51800 0.96637 0.09990 0.00635 Hamerly Julia\n14.01900 1.13100 0.07912 0.00646 Elkan Julia\n9.97000 1.14600 0.10834 0.00704 Yinyang Julia\n1,430.00000 146.00000 5.77000 0.34400 Sklearn KMeans Python\n30.10000 3.75000 0.61300 0.20100 Sklearn MiniBatchKMeans Python\n218.20000 15.51000 0.73370 0.01947 Knor R","category":"page"},{"location":"","page":"Home","title":"Home","text":"_________________________________________________________________________________________________________","category":"page"},{"location":"#Release-History","page":"Home","title":"Release History","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"0.1.0 Initial release.\n0.1.1 Added interface for MLJ.\n0.1.2 Added Elkan algorithm.\n0.1.3 Faster & optimized execution.\n0.1.4 Bug fixes.\n0.1.5 Added Yinyang algorithm.\n0.1.6 Added support for weighted k-means; Added Coreset algorithm; improved support for different types of the design matrix.\n0.1.7 Added Yinyang and Coreset support in MLJ interface; added weights support in MLJ; added RNG seed support in MLJ interface and through all algorithms; added metric support.\n0.1.8 Minor cleanup\n0.1.9 Added travis support for Julia 1.5\n0.2.0 Updated MLJ Interface\n0.2.1 Mini-batch implementation","category":"page"},{"location":"#Contributing","page":"Home","title":"Contributing","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Ultimately, we see this package as potentially the one-stop-shop for everything related to KMeans algorithm and its speed up variants. We are open to new implementations and ideas from anyone interested in this project.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Detailed contribution guidelines will be added in upcoming releases.","category":"page"},{"location":"","page":"Home","title":"Home","text":"<!â€“- TODO: Contribution Guidelines â€“->","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"Modules = [ParallelKMeans]","category":"page"},{"location":"#ParallelKMeans.AbstractKMeansAlg","page":"Home","title":"ParallelKMeans.AbstractKMeansAlg","text":"AbstractKMeansAlg\n\nAbstract base type inherited by all sub-KMeans algorithms.\n\n\n\n\n\n","category":"type"},{"location":"#ParallelKMeans.ClusteringResult","page":"Home","title":"ParallelKMeans.ClusteringResult","text":"ClusteringResult\n\nBase type for the output of clustering algorithm.\n\n\n\n\n\n","category":"type"},{"location":"#ParallelKMeans.Coreset","page":"Home","title":"ParallelKMeans.Coreset","text":"Coreset()\n\nCoreset algorithm implementation, based on \"Lucic, Mario & Bachem, Olivier & Krause, Andreas. (2015). Strong Coresets for Hard and Soft Bregman Clustering with Applications to Exponential Family Mixtures.\"\n\nCoreset supports following arguments:\n\nm: default 100, subsample size\nalg: default Lloyd(), algorithm used to clusterize sample\n\nIt can be used directly in kmeans function\n\nX = rand(30, 100_000)   # 100_000 random points in 30 dimensions\n\n# 3 clusters, Coreset algorithm with default Lloyd algorithm and 100 subsamples\nkmeans(Coreset(), X, 3)\n\n# 3 clusters, Coreset algorithm with Hamerly algorithm and 500 subsamples\nkmeans(Coreset(m = 500, alg = Hamerly()), X, 3)\nkmeans(Coreset(500, Hamerly()), X, 3)\n\n# alternatively short form can be used for defining subsample size or algorithm only\nkmeans(Coreset(500), X, 3) # sample of the size 500, Lloyd clustering algorithm\nkmeans(Coreset(Hamerly()), X, 3) # sample of the size 100, Hamerly clustering algorithm\n\n\n\n\n\n","category":"type"},{"location":"#ParallelKMeans.Elkan","page":"Home","title":"ParallelKMeans.Elkan","text":"Elkan()\n\nElkan algorithm implementation, based on \"Charles Elkan. 2003. Using the triangle inequality to accelerate k-means. In Proceedings of the Twentieth International Conference on International Conference on Machine Learning (ICMLâ€™03). AAAI Press, 147â€“153.\"\n\nThis algorithm provides much faster convergence than Lloyd algorithm especially for high dimensional data. It can be used directly in kmeans function\n\nX = rand(30, 100_000)   # 100_000 random points in 30 dimensions\n\nkmeans(Elkan(), X, 3) # 3 clusters, Elkan algorithm\n\n\n\n\n\n","category":"type"},{"location":"#ParallelKMeans.Hamerly","page":"Home","title":"ParallelKMeans.Hamerly","text":"Hamerly()\n\nHamerly algorithm implementation, based on \"Hamerly, Greg. (2010). Making k-means Even Faster.  Proceedings of the 2010 SIAM International Conference on Data Mining. 130-140. 10.1137/1.9781611972801.12.\"\n\nThis algorithm provides much faster convergence than Lloyd algorithm with realtively small increase in memory footprint. It is especially suitable for low to medium dimensional input data.\n\nIt can be used directly in kmeans function\n\nX = rand(30, 100_000)   # 100_000 random points in 30 dimensions\n\nkmeans(Hamerly(), X, 3) # 3 clusters, Hamerly algorithm\n\n\n\n\n\n","category":"type"},{"location":"#ParallelKMeans.KMeans","page":"Home","title":"ParallelKMeans.KMeans","text":"ParallelKMeans model constructed by the user.\nSee also the [package documentation](https://pydatablog.github.io/ParallelKMeans.jl/stable).\n\n\n\n\n\n","category":"type"},{"location":"#ParallelKMeans.KmeansResult","page":"Home","title":"ParallelKMeans.KmeansResult","text":"KmeansResult{C,D<:Real,WC<:Real} <: ClusteringResult\n\nThe output of kmeans and kmeans!.\n\nType parameters\n\nC<:AbstractMatrix{<:AbstractFloat}: type of the centers matrix\nD<:Real: type of the assignment cost\nWC<:Real: type of the cluster weight\n\nC is the type of centers, an (abstract) matrix of size (d x k)\n\nD is the type of pairwise distance computation from points to cluster centers\n\nWC is the type of cluster weights, either Int (in the case where points are\n\nunweighted) or eltype(weights) (in the case where points are weighted).\n\n\n\n\n\n","category":"type"},{"location":"#ParallelKMeans.Lloyd","page":"Home","title":"ParallelKMeans.Lloyd","text":"Lloyd <: AbstractKMeansAlg\n\nBasic algorithm for k-means calculation.\n\n\n\n\n\n","category":"type"},{"location":"#ParallelKMeans.MiniBatch","page":"Home","title":"ParallelKMeans.MiniBatch","text":"MiniBatch(b::Int)\n`b` represents the size of the batch which should be sampled.\n\nSculley et al. 2007 Mini batch k-means algorithm implementation.\n\nX = rand(30, 100_000)  # 100_000 random points in 30 dimensions\n\nkmeans(MiniBatch(100), X, 3)  # 3 clusters, MiniBatch algorithm with 100 batch samples at each iteration\n\n\n\n\n\n","category":"type"},{"location":"#ParallelKMeans.Yinyang","page":"Home","title":"ParallelKMeans.Yinyang","text":"Yinyang()\n\nYinyang algorithm implementation, based on \"Yufei Ding et al. 2015. Yinyang K-Means: A Drop-In Replacement of the Classic K-Means with Consistent Speedup. Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015\"\n\nGenerally it outperform Hamerly algorithm and has roughly the same time as Elkan algorithm with much lower memory consumption.\n\nYinyang supports following arguments: auto: Bool, indicates whether to perform automated or manual grouping group_size: Int, estimation of average number of clusters per group. Lower numbers corresponds to higher calculation speed and higher memory consumption and vice versa.\n\nIt can be used directly in kmeans function\n\nX = rand(30, 100_000)   # 100_000 random points in 30 dimensions\n\n# 3 clusters, Yinyang algorithm, with deault 7 group_size\nkmeans(Yinyang(), X, 3)\n\n# Following are equivalent\n# 3 clusters, Yinyang algorithm with 10 group_size\nkmeans(Yinyang(group_size = 10), X, 3)\nkmeans(Yinyang(10), X, 3)\n\n# One group with the size of the number of points\nkmeans(Yinyang(auto = false), X, 3)\nkmeans(Yinyang(false), X, 3)\n\n# Chinese writing can be used\nkmeans(é˜´é˜³(), X, 3)\n\n\n\n\n\n","category":"type"},{"location":"#MLJModelInterface.fit-Tuple{ParallelKMeans.KMeans,Int64,Any}","page":"Home","title":"MLJModelInterface.fit","text":"Fit the specified ParallelKMeans model constructed by the user.\n\nSee also the [package documentation](https://pydatablog.github.io/ParallelKMeans.jl/stable).\n\n\n\n\n\n","category":"method"},{"location":"#ParallelKMeans.chunk_colwise-NTuple{8,Any}","page":"Home","title":"ParallelKMeans.chunk_colwise","text":"chunk_colwise!(target, x, y, i, weights, r, idx)\n\nUtility function for the calculation of the weighted distance between points x and centroid vector y[:, i]. UnitRange argument r select subarray of original design matrix x that is going to be processed.\n\n\n\n\n\n","category":"method"},{"location":"#ParallelKMeans.chunk_initialize-Tuple{Hamerly,Any,Any,Any,Any,Any,Any,Any}","page":"Home","title":"ParallelKMeans.chunk_initialize","text":"chunk_initialize(alg::Hamerly, containers, centroids, X, weights, metric, r, idx)\n\nInitial calulation of all bounds and points labeling.\n\n\n\n\n\n","category":"method"},{"location":"#ParallelKMeans.chunk_update_bounds-Tuple{Hamerly,Any,Any,Any,Any,Any,Distances.Euclidean,Any,Any}","page":"Home","title":"ParallelKMeans.chunk_update_bounds","text":"chunk_update_bounds(alg::Hamerly, containers, r1, r2, pr1, pr2, metric::Euclidean, r, idx)\n\nUpdates upper and lower bounds of point distance to the centers, with regard to the centers movement when metric is Euclidean.\n\n\n\n\n\n","category":"method"},{"location":"#ParallelKMeans.chunk_update_bounds-Tuple{Hamerly,Any,Any,Any,Any,Any,Distances.Metric,Any,Any}","page":"Home","title":"ParallelKMeans.chunk_update_bounds","text":"chunk_update_bounds(alg::Hamerly, containers, r1, r2, pr1, pr2, metric::Metric, r, idx)\n\nUpdates upper and lower bounds of point distance to the centers, with regard to the centers movement when metric is Euclidean.\n\n\n\n\n\n","category":"method"},{"location":"#ParallelKMeans.chunk_update_centroids-Tuple{Hamerly,Any,Any,Any,Any,Any,Any,Any}","page":"Home","title":"ParallelKMeans.chunk_update_centroids","text":"chunk_update_centroids(alg::Hamerly, containers, centroids, X, weights, metric, r, idx)\n\nDetailed description of this function can be found in the original paper. It iterates through all points and tries to skip some calculation using known upper and lower bounds of distances from point to centers. If it fails to skip than it fall back to generic point_all_centers! function.\n\n\n\n\n\n","category":"method"},{"location":"#ParallelKMeans.create_containers-Tuple{Lloyd,Any,Any,Any,Any,Any}","page":"Home","title":"ParallelKMeans.create_containers","text":"create_containers(::Lloyd, k, nrow, ncol, n_threads)\n\nInternal function for the creation of all necessary intermidiate structures.\n\ncentroids_new - container which holds new positions of centroids\ncentroids_cnt - container which holds number of points for each centroid\nlabels - vector which holds labels of corresponding points\n\n\n\n\n\n","category":"method"},{"location":"#ParallelKMeans.create_containers-Tuple{MiniBatch,Any,Any,Any,Any,Any}","page":"Home","title":"ParallelKMeans.create_containers","text":"create_containers(::MiniBatch, k, nrow, ncol, n_threads)\n\nInternal function for the creation of all necessary intermidiate structures.\n\ncentroids_new - container which holds new positions of centroids\nlabels - vector which holds labels of corresponding points\nsum_of_squares - vector which holds the sum of squares values for each thread\nbatch_rand_idx - vector which holds the selected batch indices\n\n\n\n\n\n","category":"method"},{"location":"#ParallelKMeans.distance-NTuple{5,Any}","page":"Home","title":"ParallelKMeans.distance","text":"distance(metric, X1, X2, i1, i2)\n\nAllocationless calculation of distance between vectors X1[:, i1] and X2[:, i2] defined by the supplied distance metric.\n\n\n\n\n\n","category":"method"},{"location":"#ParallelKMeans.distance-Tuple{Distances.Euclidean,Any,Any,Any,Any}","page":"Home","title":"ParallelKMeans.distance","text":"distance(X1, X2, i1, i2)\n\nAllocationless calculation of square eucledean distance between vectors X1[:, i1] and X2[:, i2]\n\n\n\n\n\n","category":"method"},{"location":"#ParallelKMeans.double_argmax-Union{Tuple{AbstractArray{T,1}}, Tuple{T}} where T","page":"Home","title":"ParallelKMeans.double_argmax","text":"double_argmax(p)\n\nFinds maximum and next after maximum arguments.\n\n\n\n\n\n","category":"method"},{"location":"#ParallelKMeans.kmeans!","page":"Home","title":"ParallelKMeans.kmeans!","text":"kmeans!(alg::AbstractKMeansAlg, containers, design_matrix, k; n_threads = nthreads(), k_init=\"k-means++\", max_iters=300, tol=1e-6, verbose=true)\n\nMutable version of kmeans function. Definition of arguments and results can be found in kmeans.\n\nArgument containers represent algorithm specific containers, such as labels, intermidiate centroids and so on, which are used during calculations.\n\n\n\n\n\n","category":"function"},{"location":"#ParallelKMeans.kmeans-Tuple{ParallelKMeans.AbstractKMeansAlg,Any,Any}","page":"Home","title":"ParallelKMeans.kmeans","text":"kmeans([alg::AbstractKMeansAlg,] design_matrix, k; n_threads = nthreads(),\nk_init=\"k-means++\", max_iters=300, tol=1e-6, verbose=true, rng = Random.GLOBAL_RNG)\n\nThis main function employs the K-means algorithm to cluster all examples in the training data (design_matrix) into k groups using either the k-means++ or random initialisation technique for selecting the initial centroids.\n\nAt the end of the number of iterations specified (max_iters), convergence is achieved if difference between the current and last cost objective is less than the tolerance level (tol). An error is thrown if convergence fails.\n\nArguments:\n\nalg defines one of the algorithms used to calculate k-means. This\n\nargument can be omitted, by default Lloyd algorithm is used.\n\nn_threads defines number of threads used for calculations, by default it is equal\n\nto the Threads.nthreads() which is defined by JULIA_NUM_THREADS environmental variable. For small size design matrices it make sense to set this argument to 1 in order to avoid overhead of threads generation.\n\nk_init is one of the algorithms used for initialization. By default k-means++ algorithm is used,\n\nalternatively one can use rand to choose random points for init.\n\nmax_iters is the maximum number of iterations\ntol defines tolerance for early stopping.\nverbose is verbosity level. Details of operations can be either printed or not by setting verbose accordingly.\n\nA KmeansResult structure representing labels, centroids, and sum_squares is returned.\n\n\n\n\n\n","category":"method"},{"location":"#ParallelKMeans.move_centers-Tuple{Hamerly,Any,Any,Any}","page":"Home","title":"ParallelKMeans.move_centers","text":"move_centers(::Hamerly, containers, centroids, metric)\n\nCalculates new positions of centers and distance they have moved. Results are stored in centroids and p respectively.\n\n\n\n\n\n","category":"method"},{"location":"#ParallelKMeans.point_all_centers!-NTuple{5,Any}","page":"Home","title":"ParallelKMeans.point_all_centers!","text":"point_all_centers!(containers, centroids, X, i, metric)\n\nCalculates new labels and upper and lower bounds for all points.\n\n\n\n\n\n","category":"method"},{"location":"#ParallelKMeans.point_all_centers!-Tuple{Yinyang,Any,Any,Any,Any,Any}","page":"Home","title":"ParallelKMeans.point_all_centers!","text":"point_all_centers!(containers, centroids, X, i)\n\nCalculates new labels and upper and lower bounds for all points.\n\n\n\n\n\n","category":"method"},{"location":"#ParallelKMeans.reassign_labels-NTuple{4,Any}","page":"Home","title":"ParallelKMeans.reassign_labels","text":"reassign_labels(DMatrix, metric, labels, centres)\n\nAn internal function to relabel DMatrix based on centres and metric.\n\n\n\n\n\n","category":"method"},{"location":"#ParallelKMeans.smart_init","page":"Home","title":"ParallelKMeans.smart_init","text":"smart_init(X, k; init=\"k-means++\")\n\nThis function handles the random initialisation of the centroids from the design matrix (X) and desired groups (k) that a user supplies.\n\nk-means++ algorithm is used by default with the normal random selection of centroids from X used if any other string is attempted.\n\nA named tuple representing centroids and indices respecitively is returned.\n\n\n\n\n\n","category":"function"},{"location":"#ParallelKMeans.splitter-Tuple{Any,Any}","page":"Home","title":"ParallelKMeans.splitter","text":"spliiter(n, k)\n\nInternal utility function, splits 1:n sequence to k chunks of approximately same size.\n\n\n\n\n\n","category":"method"},{"location":"#ParallelKMeans.sum_of_squares-NTuple{8,Any}","page":"Home","title":"ParallelKMeans.sum_of_squares","text":"sum_of_squares(x, labels, centre, k)\n\nThis function computes the total sum of squares based on the assigned (labels) design matrix(x), centroids (centre), and the number of desired groups (k).\n\nA Float type representing the computed metric is returned.\n\n\n\n\n\n","category":"method"},{"location":"#ParallelKMeans.update_containers-Tuple{Hamerly,Any,Any,Any,Any}","page":"Home","title":"ParallelKMeans.update_containers","text":"update_containers(::Hamerly, containers, centroids, n_threads, metric)\n\nCalculates minimum distances from centers to each other.\n\n\n\n\n\n","category":"method"},{"location":"#ParallelKMeans.@parallelize-Tuple{Any,Any,Any}","page":"Home","title":"ParallelKMeans.@parallelize","text":"@parallelize(n_threads, ncol, f)\n\nParallelize function and run it over n_threads. Function should require following conditions:\n\nIt should not return any values.\nIt should accept parameters two parameters at the end of the argument list. First\n\naccepted parameter is range, which defines chunk used in calculations. Second parameter is idx which defines id of the container where results can be stored.\n\nncol argument defines range 1:ncol which is sliced in n_threads chunks.\n\n\n\n\n\n","category":"macro"}]
}
