<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Home · ParallelKMeans.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">ParallelKMeans.jl</span></div><form class="docs-search" action="search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li class="is-active"><a class="tocitem" href>Home</a><ul class="internal"><li><a class="tocitem" href="#Motivation-1"><span>Motivation</span></a></li><li><a class="tocitem" href="#K-Means-Algorithm-Implementation-Notes-1"><span>K-Means Algorithm Implementation Notes</span></a></li><li><a class="tocitem" href="#Installation-1"><span>Installation</span></a></li><li><a class="tocitem" href="#Features-1"><span>Features</span></a></li><li><a class="tocitem" href="#Pending-Features-1"><span>Pending Features</span></a></li><li><a class="tocitem" href="#How-To-Use-1"><span>How To Use</span></a></li><li><a class="tocitem" href="#Benchmarks-1"><span>Benchmarks</span></a></li><li><a class="tocitem" href="#Release-History-1"><span>Release History</span></a></li><li><a class="tocitem" href="#Contributing-1"><span>Contributing</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Home</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Home</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/PyDataBlog/ParallelKMeans.jl/blob/master/docs/src/index.md#L" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="[ParallelKMeans.jl-Package](https://github.com/PyDataBlog/ParallelKMeans.jl)-1"><a class="docs-heading-anchor" href="#[ParallelKMeans.jl-Package](https://github.com/PyDataBlog/ParallelKMeans.jl)-1"><a href="https://github.com/PyDataBlog/ParallelKMeans.jl">ParallelKMeans.jl Package</a></a><a class="docs-heading-anchor-permalink" href="#[ParallelKMeans.jl-Package](https://github.com/PyDataBlog/ParallelKMeans.jl)-1" title="Permalink"></a></h1><h2 id="Motivation-1"><a class="docs-heading-anchor" href="#Motivation-1">Motivation</a><a class="docs-heading-anchor-permalink" href="#Motivation-1" title="Permalink"></a></h2><p>It&#39;s actually a funny story led to the development of this package. What started off as a personal toy project trying to re-construct the K-Means algorithm in native Julia blew up after a heated discussion on the Julia Discourse forum when I asked for Julia optimizaition tips. Long story short, Julia community is an amazing one! Andrey offered his help and together, we decided to push the speed limits of Julia with a parallel implementation of the most famous clustering algorithm. The initial results were mind blowing so we have decided to tidy up the implementation and share with the world as a maintained Julia pacakge.</p><p>Say hello to <code>ParallelKMeans</code>!</p><p>This package aims to utilize the speed of Julia and parallelization (both CPU &amp; GPU) to offer an extremely fast implementation of the K-Means clustering algorithm and its variations via a friendly interface for practioners.</p><p>In short, we hope this package will eventually mature as the &quot;one stop&quot; shop for everything K-Means on both CPUs and GPUs.</p><h2 id="K-Means-Algorithm-Implementation-Notes-1"><a class="docs-heading-anchor" href="#K-Means-Algorithm-Implementation-Notes-1">K-Means Algorithm Implementation Notes</a><a class="docs-heading-anchor-permalink" href="#K-Means-Algorithm-Implementation-Notes-1" title="Permalink"></a></h2><p>Since Julia is a column major language, the input (design matrix) expected by the package in the following format;</p><ul><li>Design matrix X of size n×m, the i-th column of X <code>(X[:, i])</code> is a single data point in n-dimensional space.</li><li>Thus, the rows of the design design matrix represents the feature space with the columns representing all the training examples in this feature space.</li></ul><p>One of the pitfalls of K-Means algorithm is that it can fall into a local minima. This implementation inherits this problem like every implementation does. As a result, it is useful in practice to restart it several times to get the correct results.</p><h2 id="Installation-1"><a class="docs-heading-anchor" href="#Installation-1">Installation</a><a class="docs-heading-anchor-permalink" href="#Installation-1" title="Permalink"></a></h2><p>You can grab the latest stable version of this package from Julia registries by simply running;</p><p><em>NB:</em> Don&#39;t forget to Julia&#39;s package manager with <code>]</code></p><pre><code class="language-julia">pkg&gt; add ParallelKMeans</code></pre><p>For the few (and selected) brave ones, one can simply grab the current experimental features by simply adding the experimental branch to your development environment after invoking the package manager with <code>]</code>:</p><pre><code class="language-julia">dev git@github.com:PyDataBlog/ParallelKMeans.jl.git</code></pre><p>Don&#39;t forget to checkout the experimental branch and you are good to go with bleeding edge features and breaks!</p><pre><code class="language-bash">git checkout experimental</code></pre><h2 id="Features-1"><a class="docs-heading-anchor" href="#Features-1">Features</a><a class="docs-heading-anchor-permalink" href="#Features-1" title="Permalink"></a></h2><ul><li>Lightening fast implementation of Kmeans clustering algorithm even on a single thread in native Julia.</li><li>Support for multi-theading implementation of K-Means clustering algorithm.</li><li>&#39;Kmeans++&#39; initialization for faster and better convergence.</li><li>Implementation of available classic and contemporary variants of the K-Means algorithm.</li></ul><h2 id="Pending-Features-1"><a class="docs-heading-anchor" href="#Pending-Features-1">Pending Features</a><a class="docs-heading-anchor-permalink" href="#Pending-Features-1" title="Permalink"></a></h2><ul><li>[X] Implementation of <a href="https://www.researchgate.net/publication/220906984_Making_k-means_Even_Faster">Hamerly implementation</a>.</li><li>[X] Interface for inclusion in Alan Turing Institute&#39;s <a href="https://github.com/alan-turing-institute/MLJModels.jl#who-is-this-repo-for">MLJModels</a>.</li><li>[X] Full Implementation of Triangle inequality based on <a href="https://www.aaai.org/Papers/ICML/2003/ICML03-022.pdf">Elkan - 2003 Using the Triangle Inequality to Accelerate K-Means&quot;</a>.</li><li>[ ] Implementation of <a href="http://cs.baylor.edu/~hamerly/papers/sdm2016_rysavy_hamerly.pdf">Geometric methods to accelerate k-means algorithm</a>.</li><li>[ ] Support for other distance metrics supported by <a href="https://github.com/JuliaStats/Distances.jl#supported-distances">Distances.jl</a>.</li><li>[ ] Native support for tabular data inputs outside of MLJModels&#39; interface.</li><li>[ ] Refactoring and finalizaiton of API desgin.</li><li>[ ] GPU support.</li><li>[ ] Implementation of other K-Means algorithm variants based on recent literature.</li><li>[ ] Optimization of code base.</li><li>[ ] Improved Documentation</li><li>[ ] More benchmark tests.</li></ul><h2 id="How-To-Use-1"><a class="docs-heading-anchor" href="#How-To-Use-1">How To Use</a><a class="docs-heading-anchor-permalink" href="#How-To-Use-1" title="Permalink"></a></h2><p>Taking advantage of Julia&#39;s brilliant multiple dispatch system, the package exposes users to a very easy to use API.</p><pre><code class="language-julia">using ParallelKMeans

# Uses all available CPU cores by default
multi_results = kmeans(X, 3; max_iters=300)

# Use only 1 core of CPU
results = kmeans(X, 3; n_threads=1, max_iters=300)</code></pre><p>The main design goal is to offer all available variations of the KMeans algorithm to end users as composable elements. By default, Lloyd&#39;s implementation is used but users can specify different variations of the KMeans clustering algorithm via this interface;</p><pre><code class="language-julia">some_results = kmeans([algo], input_matrix, k; kwargs)

# example
r = kmeans(Lloyd(), X, 3)  # same result as the default</code></pre><pre><code class="language-julia"># r contains all the learned artifacts which can be accessed as;
r.centers               # cluster centers (d x k)
r.assignments           # label assignments (n)
r.totalcost             # total cost (i.e. objective)
r.iterations            # number of elapsed iterations
r.converged             # whether the procedure converged</code></pre><h3 id="Supported-KMeans-algorithm-variations-1"><a class="docs-heading-anchor" href="#Supported-KMeans-algorithm-variations-1">Supported KMeans algorithm variations</a><a class="docs-heading-anchor-permalink" href="#Supported-KMeans-algorithm-variations-1" title="Permalink"></a></h3><ul><li><a href="https://cs.nyu.edu/~roweis/csc2515-2006/readings/lloyd57.pdf">Lloyd()</a></li><li><a href="https://www.researchgate.net/publication/220906984_Making_k-means_Even_Faster">Hamerly()</a></li><li><a href="https://www.aaai.org/Papers/ICML/2003/ICML03-022.pdf">Elkan()</a></li><li><a href="http://cs.baylor.edu/~hamerly/papers/sdm2016_rysavy_hamerly.pdf">Geometric()</a> - (Coming soon)</li><li><a href="https://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf">MiniBatch()</a> - (Coming soon)</li></ul><h3 id="Practical-Usage-Examples-1"><a class="docs-heading-anchor" href="#Practical-Usage-Examples-1">Practical Usage Examples</a><a class="docs-heading-anchor-permalink" href="#Practical-Usage-Examples-1" title="Permalink"></a></h3><p>Some of the common usage examples of this package are as follows:</p><h4 id="Clustering-With-A-Desired-Number-Of-Groups-1"><a class="docs-heading-anchor" href="#Clustering-With-A-Desired-Number-Of-Groups-1">Clustering With A Desired Number Of Groups</a><a class="docs-heading-anchor-permalink" href="#Clustering-With-A-Desired-Number-Of-Groups-1" title="Permalink"></a></h4><pre><code class="language-julia">using ParallelKMeans, RDatasets, Plots

# load the data
iris = dataset(&quot;datasets&quot;, &quot;iris&quot;);

# features to use for clustering
features = collect(Matrix(iris[:, 1:4])&#39;);

# various artificats can be accessed from the result ie assigned labels, cost value etc
result = kmeans(features, 3);

# plot with the point color mapped to the assigned cluster index
scatter(iris.PetalLength, iris.PetalWidth, marker_z=result.assignments,
        color=:lightrainbow, legend=false)
</code></pre><p><img src="iris_example.jpg" alt="Image description"/></p><h4 id="Elbow-Method-For-The-Selection-Of-optimal-number-of-clusters-1"><a class="docs-heading-anchor" href="#Elbow-Method-For-The-Selection-Of-optimal-number-of-clusters-1">Elbow Method For The Selection Of optimal number of clusters</a><a class="docs-heading-anchor-permalink" href="#Elbow-Method-For-The-Selection-Of-optimal-number-of-clusters-1" title="Permalink"></a></h4><pre><code class="language-julia">using ParallelKMeans

# Single Thread Implementation of Lloyd&#39;s Algorithm
b = [ParallelKMeans.kmeans(X, i, n_threads=1; tol=1e-6, max_iters=300, verbose=false).totalcost for i = 2:10]

# Multi Thread Implementation of Lloyd&#39;s Algorithm by default
c = [ParallelKMeans.kmeans(X, i; tol=1e-6, max_iters=300, verbose=false).totalcost for i = 2:10]
</code></pre><h2 id="Benchmarks-1"><a class="docs-heading-anchor" href="#Benchmarks-1">Benchmarks</a><a class="docs-heading-anchor-permalink" href="#Benchmarks-1" title="Permalink"></a></h2><p>Currently, this package is benchmarked against similar implementation in both Python and Julia. All reproducible benchmarks can be found in <a href="https://github.com/PyDataBlog/ParallelKMeans.jl/tree/master/extras">ParallelKMeans/extras</a> directory. More tests in various languages are planned beyond the initial release version (<code>0.1.0</code>).</p><p><em>Note</em>: All benchmark tests are made on the same computer to help eliminate any bias.</p><p>Currently, the benchmark speed tests are based on the search for optimal number of clusters using the <a href="https://en.wikipedia.org/wiki/Elbow_method_(clustering)">Elbow Method</a> since this is a practical use case for most practioners employing the K-Means algorithm.</p><h3 id="Benchmark-Results-1"><a class="docs-heading-anchor" href="#Benchmark-Results-1">Benchmark Results</a><a class="docs-heading-anchor-permalink" href="#Benchmark-Results-1" title="Permalink"></a></h3><p><img src="benchmark_image.png" alt="benchmark_image.png"/></p><p>_________________________________________________________________________________________________________</p><table><tr><th style="text-align: center">1 million (ms)</th><th style="text-align: center">100k (ms)</th><th style="text-align: center">10k (ms)</th><th style="text-align: center">1k (ms)</th><th style="text-align: center">package</th><th style="text-align: center">language</th></tr><tr><td style="text-align: center">666840</td><td style="text-align: center">34034</td><td style="text-align: center">709.049</td><td style="text-align: center">17.686</td><td style="text-align: center">Clustering.jl</td><td style="text-align: center">Julia</td></tr><tr><td style="text-align: center">21730</td><td style="text-align: center">2975</td><td style="text-align: center">163.771</td><td style="text-align: center">6.444</td><td style="text-align: center">ParallelKMeans Lloyd</td><td style="text-align: center">Julia</td></tr><tr><td style="text-align: center">11784</td><td style="text-align: center">1339</td><td style="text-align: center">94.233</td><td style="text-align: center">6.6</td><td style="text-align: center">ParallelKMeans Hamerly</td><td style="text-align: center">Julia</td></tr><tr><td style="text-align: center">17591</td><td style="text-align: center">1074</td><td style="text-align: center">81.995</td><td style="text-align: center">6.953</td><td style="text-align: center">ParallelKMeans Elkan</td><td style="text-align: center">Julia</td></tr><tr><td style="text-align: center">1430000</td><td style="text-align: center">146000</td><td style="text-align: center">5770</td><td style="text-align: center">344</td><td style="text-align: center">Sklearn Kmeans</td><td style="text-align: center">Python</td></tr><tr><td style="text-align: center">30100</td><td style="text-align: center">3750</td><td style="text-align: center">613</td><td style="text-align: center">201</td><td style="text-align: center">Sklearn MiniBatchKmeans</td><td style="text-align: center">Python</td></tr><tr><td style="text-align: center">218200</td><td style="text-align: center">15510</td><td style="text-align: center">733.7</td><td style="text-align: center">19.47</td><td style="text-align: center">Knor</td><td style="text-align: center">R</td></tr></table><p>_________________________________________________________________________________________________________</p><h2 id="Release-History-1"><a class="docs-heading-anchor" href="#Release-History-1">Release History</a><a class="docs-heading-anchor-permalink" href="#Release-History-1" title="Permalink"></a></h2><ul><li>0.1.0 Initial release.</li><li>0.1.1 Added interface for MLJ.</li><li>0.1.2 Added Elkan algorithm.</li><li>0.1.3 Faster &amp; optimized execution.</li></ul><h2 id="Contributing-1"><a class="docs-heading-anchor" href="#Contributing-1">Contributing</a><a class="docs-heading-anchor-permalink" href="#Contributing-1" title="Permalink"></a></h2><p>Ultimately, we see this package as potentially the one stop shop for everything related to KMeans algorithm and its speed up variants. We are open to new implementations and ideas from anyone interested in this project.</p><p>Detailed contribution guidelines will be added in upcoming releases.</p><p>&lt;!–- TODO: Contribution Guidelines –-&gt;</p><ul><li><a href="#ParallelKMeans.AbstractKMeansAlg"><code>ParallelKMeans.AbstractKMeansAlg</code></a></li><li><a href="#ParallelKMeans.ClusteringResult"><code>ParallelKMeans.ClusteringResult</code></a></li><li><a href="#ParallelKMeans.Elkan"><code>ParallelKMeans.Elkan</code></a></li><li><a href="#ParallelKMeans.Hamerly"><code>ParallelKMeans.Hamerly</code></a></li><li><a href="#ParallelKMeans.KmeansResult"><code>ParallelKMeans.KmeansResult</code></a></li><li><a href="#ParallelKMeans.Lloyd"><code>ParallelKMeans.Lloyd</code></a></li><li><a href="#MLJModelInterface.fit-Tuple{ParallelKMeans.KMeans,Int64,Any}"><code>MLJModelInterface.fit</code></a></li><li><a href="#ParallelKMeans.chunk_colwise!-NTuple{4,Any}"><code>ParallelKMeans.chunk_colwise!</code></a></li><li><a href="#ParallelKMeans.chunk_initialize-Tuple{Hamerly,Any,Any,Any,Any,Any}"><code>ParallelKMeans.chunk_initialize</code></a></li><li><a href="#ParallelKMeans.chunk_update_bounds-Tuple{Hamerly,Any,Any,Any,Any,Any,Any,Any}"><code>ParallelKMeans.chunk_update_bounds</code></a></li><li><a href="#ParallelKMeans.chunk_update_centroids-Tuple{Hamerly,Any,Any,Any,Any,Any}"><code>ParallelKMeans.chunk_update_centroids</code></a></li><li><a href="#ParallelKMeans.colwise!"><code>ParallelKMeans.colwise!</code></a></li><li><a href="#ParallelKMeans.create_containers-Tuple{Lloyd,Any,Any,Any,Any}"><code>ParallelKMeans.create_containers</code></a></li><li><a href="#ParallelKMeans.distance-NTuple{4,Any}"><code>ParallelKMeans.distance</code></a></li><li><a href="#ParallelKMeans.double_argmax-Tuple{Any}"><code>ParallelKMeans.double_argmax</code></a></li><li><a href="#ParallelKMeans.kmeans-Tuple{Any,Any,Any}"><code>ParallelKMeans.kmeans</code></a></li><li><a href="#ParallelKMeans.kmeans!-Tuple{Lloyd,Any,Any,Any}"><code>ParallelKMeans.kmeans!</code></a></li><li><a href="#ParallelKMeans.move_centers-Tuple{Hamerly,Any,Any}"><code>ParallelKMeans.move_centers</code></a></li><li><a href="#ParallelKMeans.point_all_centers!-NTuple{4,Any}"><code>ParallelKMeans.point_all_centers!</code></a></li><li><a href="#ParallelKMeans.smart_init"><code>ParallelKMeans.smart_init</code></a></li><li><a href="#ParallelKMeans.splitter-Tuple{Any,Any}"><code>ParallelKMeans.splitter</code></a></li><li><a href="#ParallelKMeans.sum_of_squares-NTuple{6,Any}"><code>ParallelKMeans.sum_of_squares</code></a></li><li><a href="#ParallelKMeans.update_containers-Tuple{Hamerly,Any,Any,Any}"><code>ParallelKMeans.update_containers</code></a></li><li><a href="#ParallelKMeans.@parallelize-Tuple{Any,Any,Any}"><code>ParallelKMeans.@parallelize</code></a></li></ul><article class="docstring"><header><a class="docstring-binding" id="ParallelKMeans.Elkan" href="#ParallelKMeans.Elkan"><code>ParallelKMeans.Elkan</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Elkan()</code></pre><p>Elkan algorithm implementation, based on &quot;Charles Elkan. 2003. Using the triangle inequality to accelerate k-means. In Proceedings of the Twentieth International Conference on International Conference on Machine Learning (ICML’03). AAAI Press, 147–153.&quot;</p><p>This algorithm provides much faster convergence than Lloyd algorithm especially for high dimensional data. It can be used directly in <code>kmeans</code> function</p><pre><code class="language-julia">X = rand(30, 100_000)   # 100_000 random points in 30 dimensions

kmeans(Elkan(), X, 3) # 3 clusters, Elkan algorithm</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PyDataBlog/ParallelKMeans.jl/blob/a7107daf5ba143382fedf509db2f578cac17f37d/src/elkan.jl#LL1-L18">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ParallelKMeans.Hamerly" href="#ParallelKMeans.Hamerly"><code>ParallelKMeans.Hamerly</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Hamerly()</code></pre><p>Hamerly algorithm implementation, based on &quot;Hamerly, Greg. (2010). Making k-means Even Faster.  Proceedings of the 2010 SIAM International Conference on Data Mining. 130-140. 10.1137/1.9781611972801.12.&quot;</p><p>This algorithm provides much faster convergence than Lloyd algorithm with realtively small increase in memory footprint. It is especially suitable for low to medium dimensional input data.</p><p>It can be used directly in <code>kmeans</code> function</p><pre><code class="language-julia">X = rand(30, 100_000)   # 100_000 random points in 30 dimensions

kmeans(Hamerly(), X, 3) # 3 clusters, Hamerly algorithm</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PyDataBlog/ParallelKMeans.jl/blob/a7107daf5ba143382fedf509db2f578cac17f37d/src/hamerly.jl#LL1-L17">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ParallelKMeans.Lloyd" href="#ParallelKMeans.Lloyd"><code>ParallelKMeans.Lloyd</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Lloyd &lt;: AbstractKMeansAlg</code></pre><p>Basic algorithm for k-means calculation.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PyDataBlog/ParallelKMeans.jl/blob/a7107daf5ba143382fedf509db2f578cac17f37d/src/lloyd.jl#LL1-L5">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ParallelKMeans.kmeans-Tuple{Any,Any,Any}" href="#ParallelKMeans.kmeans-Tuple{Any,Any,Any}"><code>ParallelKMeans.kmeans</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">Kmeans([alg::AbstractKMeansAlg,] design_matrix, k; n_threads = nthreads(), k_init=&quot;k-means++&quot;, max_iters=300, tol=1e-6, verbose=true)</code></pre><p>This main function employs the K-means algorithm to cluster all examples in the training data (design_matrix) into k groups using either the <code>k-means++</code> or random initialisation technique for selecting the initial centroids.</p><p>At the end of the number of iterations specified (max_iters), convergence is achieved if difference between the current and last cost objective is less than the tolerance level (tol). An error is thrown if convergence fails.</p><p>Arguments:</p><ul><li><code>alg</code> defines one of the algorithms used to calculate <code>k-means</code>. This</li></ul><p>argument can be omitted, by default Lloyd algorithm is used.</p><ul><li><code>n_threads</code> defines number of threads used for calculations, by default it is equal</li></ul><p>to the <code>Threads.nthreads()</code> which is defined by <code>JULIA_NUM_THREADS</code> environmental variable. For small size design matrices it make sense to set this argument to 1 in order to avoid overhead of threads generation.</p><ul><li><code>k_init</code> is one of the algorithms used for initialization. By default <code>k-means++</code> algorithm is used,</li></ul><p>alternatively one can use <code>rand</code> to choose random points for init.</p><ul><li><code>max_iters</code> is the maximum number of iterations</li><li><code>tol</code> defines tolerance for early stopping.</li><li><code>verbose</code> is verbosity level. Details of operations can be either printed or not by setting verbose accordingly.</li></ul><p>A <code>KmeansResult</code> structure representing labels, centroids, and sum_squares is returned.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PyDataBlog/ParallelKMeans.jl/blob/a7107daf5ba143382fedf509db2f578cac17f37d/src/kmeans.jl#LL124-L150">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ParallelKMeans.AbstractKMeansAlg" href="#ParallelKMeans.AbstractKMeansAlg"><code>ParallelKMeans.AbstractKMeansAlg</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">AbstractKMeansAlg</code></pre><p>Abstract base type inherited by all sub-KMeans algorithms.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PyDataBlog/ParallelKMeans.jl/blob/a7107daf5ba143382fedf509db2f578cac17f37d/src/kmeans.jl#LL2-L6">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ParallelKMeans.ClusteringResult" href="#ParallelKMeans.ClusteringResult"><code>ParallelKMeans.ClusteringResult</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">ClusteringResult</code></pre><p>Base type for the output of clustering algorithm.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PyDataBlog/ParallelKMeans.jl/blob/a7107daf5ba143382fedf509db2f578cac17f37d/src/kmeans.jl#LL10-L14">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ParallelKMeans.KmeansResult" href="#ParallelKMeans.KmeansResult"><code>ParallelKMeans.KmeansResult</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">KmeansResult{C,D&lt;:Real,WC&lt;:Real} &lt;: ClusteringResult</code></pre><p>The output of <a href="#ParallelKMeans.kmeans-Tuple{Any,Any,Any}"><code>kmeans</code></a> and <a href="#ParallelKMeans.kmeans!-Tuple{Lloyd,Any,Any,Any}"><code>kmeans!</code></a>.</p><p><strong>Type parameters</strong></p><ul><li><code>C&lt;:AbstractMatrix{&lt;:AbstractFloat}</code>: type of the <code>centers</code> matrix</li><li><code>D&lt;:Real</code>: type of the assignment cost</li><li><code>WC&lt;:Real</code>: type of the cluster weight</li></ul><p><strong>C is the type of centers, an (abstract) matrix of size (d x k)</strong></p><p><strong>D is the type of pairwise distance computation from points to cluster centers</strong></p><p><strong>WC is the type of cluster weights, either Int (in the case where points are</strong></p><p><strong>unweighted) or eltype(weights) (in the case where points are weighted).</strong></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PyDataBlog/ParallelKMeans.jl/blob/a7107daf5ba143382fedf509db2f578cac17f37d/src/kmeans.jl#LL19-L31">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJModelInterface.fit-Tuple{ParallelKMeans.KMeans,Int64,Any}" href="#MLJModelInterface.fit-Tuple{ParallelKMeans.KMeans,Int64,Any}"><code>MLJModelInterface.fit</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">Fit the specified ParaKMeans model constructed by the user.

See also the [package documentation](https://pydatablog.github.io/ParallelKMeans.jl/stable).</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PyDataBlog/ParallelKMeans.jl/blob/a7107daf5ba143382fedf509db2f578cac17f37d/src/mlj_interface.jl#LL77-L81">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ParallelKMeans.chunk_colwise!-NTuple{4,Any}" href="#ParallelKMeans.chunk_colwise!-NTuple{4,Any}"><code>ParallelKMeans.chunk_colwise!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">chunk_colwise!(target, x, y, r)</code></pre><p>Utility function for calculation of the <code>colwise!(target, x, y, n_threads)</code> function. UnitRange argument <code>r</code> select subarray of original design matrix <code>x</code> that is going to be processed.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PyDataBlog/ParallelKMeans.jl/blob/a7107daf5ba143382fedf509db2f578cac17f37d/src/seeding.jl#LL43-L49">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ParallelKMeans.chunk_initialize-Tuple{Hamerly,Any,Any,Any,Any,Any}" href="#ParallelKMeans.chunk_initialize-Tuple{Hamerly,Any,Any,Any,Any,Any}"><code>ParallelKMeans.chunk_initialize</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">chunk_initialize(alg::Hamerly, containers, centroids, design_matrix, r, idx)</code></pre><p>Initial calulation of all bounds and points labeling.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PyDataBlog/ParallelKMeans.jl/blob/a7107daf5ba143382fedf509db2f578cac17f37d/src/hamerly.jl#LL132-L136">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ParallelKMeans.chunk_update_bounds-Tuple{Hamerly,Any,Any,Any,Any,Any,Any,Any}" href="#ParallelKMeans.chunk_update_bounds-Tuple{Hamerly,Any,Any,Any,Any,Any,Any,Any}"><code>ParallelKMeans.chunk_update_bounds</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">chunk_update_bounds(alg::Hamerly, containers, r1, r2, pr1, pr2, r, idx)</code></pre><p>Updates upper and lower bounds of point distance to the centers, with regard to the centers movement.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PyDataBlog/ParallelKMeans.jl/blob/a7107daf5ba143382fedf509db2f578cac17f37d/src/hamerly.jl#LL261-L265">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ParallelKMeans.chunk_update_centroids-Tuple{Hamerly,Any,Any,Any,Any,Any}" href="#ParallelKMeans.chunk_update_centroids-Tuple{Hamerly,Any,Any,Any,Any,Any}"><code>ParallelKMeans.chunk_update_centroids</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">chunk_update_centroids(::Hamerly, containers, centroids, X, r, idx)</code></pre><p>Detailed description of this function can be found in the original paper. It iterates through all points and tries to skip some calculation using known upper and lower bounds of distances from point to centers. If it fails to skip than it fall back to generic <code>point_all_centers!</code> function.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PyDataBlog/ParallelKMeans.jl/blob/a7107daf5ba143382fedf509db2f578cac17f37d/src/hamerly.jl#LL168-L174">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ParallelKMeans.colwise!" href="#ParallelKMeans.colwise!"><code>ParallelKMeans.colwise!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">colwise!(target, x, y, n_threads)</code></pre><p>Internal function for colwise calculations. Let <code>x</code> is a matrix <code>m x n</code> and <code>y</code> is a vector of the length <code>m</code>. Then the <code>colwise!</code> function computes distance between each column in <code>x</code> and <code>y</code> and store result in <code>target</code> array. Argument <code>n_threads</code> defines the number of threads used for calculation.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PyDataBlog/ParallelKMeans.jl/blob/a7107daf5ba143382fedf509db2f578cac17f37d/src/seeding.jl#LL11-L17">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ParallelKMeans.create_containers-Tuple{Lloyd,Any,Any,Any,Any}" href="#ParallelKMeans.create_containers-Tuple{Lloyd,Any,Any,Any,Any}"><code>ParallelKMeans.create_containers</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">create_containers(::Lloyd, k, nrow, ncol, n_threads)</code></pre><p>Internal function for the creation of all necessary intermidiate structures.</p><ul><li><code>centroids_new</code> - container which holds new positions of centroids</li><li><code>centroids_cnt</code> - container which holds number of points for each centroid</li><li><code>labels</code> - vector which holds labels of corresponding points</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PyDataBlog/ParallelKMeans.jl/blob/a7107daf5ba143382fedf509db2f578cac17f37d/src/lloyd.jl#LL70-L78">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ParallelKMeans.distance-NTuple{4,Any}" href="#ParallelKMeans.distance-NTuple{4,Any}"><code>ParallelKMeans.distance</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">distance(X1, X2, i1, i2)</code></pre><p>Allocationless calculation of square eucledean distance between vectors X1[:, i1] and X2[:, i2]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PyDataBlog/ParallelKMeans.jl/blob/a7107daf5ba143382fedf509db2f578cac17f37d/src/kmeans.jl#LL89-L93">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ParallelKMeans.double_argmax-Tuple{Any}" href="#ParallelKMeans.double_argmax-Tuple{Any}"><code>ParallelKMeans.double_argmax</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">double_argmax(p)</code></pre><p>Finds maximum and next after maximum arguments.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PyDataBlog/ParallelKMeans.jl/blob/a7107daf5ba143382fedf509db2f578cac17f37d/src/hamerly.jl#LL299-L303">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ParallelKMeans.kmeans!-Tuple{Lloyd,Any,Any,Any}" href="#ParallelKMeans.kmeans!-Tuple{Lloyd,Any,Any,Any}"><code>ParallelKMeans.kmeans!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">Kmeans!(alg::AbstractKMeansAlg, containers, design_matrix, k; n_threads = nthreads(), k_init=&quot;k-means++&quot;, max_iters=300, tol=1e-6, verbose=true)</code></pre><p>Mutable version of <code>kmeans</code> function. Definition of arguments and results can be found in <code>kmeans</code>.</p><p>Argument <code>containers</code> represent algorithm specific containers, such as labels, intermidiate centroids and so on, which are used during calculations.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PyDataBlog/ParallelKMeans.jl/blob/a7107daf5ba143382fedf509db2f578cac17f37d/src/lloyd.jl#LL8-L16">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ParallelKMeans.move_centers-Tuple{Hamerly,Any,Any}" href="#ParallelKMeans.move_centers-Tuple{Hamerly,Any,Any}"><code>ParallelKMeans.move_centers</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">move_centers(::Hamerly, containers, centroids)</code></pre><p>Calculates new positions of centers and distance they have moved. Results are stored in <code>centroids</code> and <code>p</code> respectively.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PyDataBlog/ParallelKMeans.jl/blob/a7107daf5ba143382fedf509db2f578cac17f37d/src/hamerly.jl#LL241-L246">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ParallelKMeans.point_all_centers!-NTuple{4,Any}" href="#ParallelKMeans.point_all_centers!-NTuple{4,Any}"><code>ParallelKMeans.point_all_centers!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">point_all_centers!(containers, centroids, X, i)</code></pre><p>Calculates new labels and upper and lower bounds for all points.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PyDataBlog/ParallelKMeans.jl/blob/a7107daf5ba143382fedf509db2f578cac17f37d/src/hamerly.jl#LL210-L214">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ParallelKMeans.smart_init" href="#ParallelKMeans.smart_init"><code>ParallelKMeans.smart_init</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">smart_init(X, k; init=&quot;k-means++&quot;)</code></pre><p>This function handles the random initialisation of the centroids from the design matrix (X) and desired groups (k) that a user supplies.</p><p><code>k-means++</code> algorithm is used by default with the normal random selection of centroids from X used if any other string is attempted.</p><p>A named tuple representing centroids and indices respecitively is returned.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PyDataBlog/ParallelKMeans.jl/blob/a7107daf5ba143382fedf509db2f578cac17f37d/src/seeding.jl#LL60-L70">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ParallelKMeans.splitter-Tuple{Any,Any}" href="#ParallelKMeans.splitter-Tuple{Any,Any}"><code>ParallelKMeans.splitter</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">spliiter(n, k)</code></pre><p>Internal utility function, splits 1:n sequence to k chunks of approximately same size.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PyDataBlog/ParallelKMeans.jl/blob/a7107daf5ba143382fedf509db2f578cac17f37d/src/seeding.jl#LL1-L5">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ParallelKMeans.sum_of_squares-NTuple{6,Any}" href="#ParallelKMeans.sum_of_squares-NTuple{6,Any}"><code>ParallelKMeans.sum_of_squares</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">sum_of_squares(x, labels, centre, k)</code></pre><p>This function computes the total sum of squares based on the assigned (labels) design matrix(x), centroids (centre), and the number of desired groups (k).</p><p>A Float type representing the computed metric is returned.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PyDataBlog/ParallelKMeans.jl/blob/a7107daf5ba143382fedf509db2f578cac17f37d/src/kmeans.jl#LL104-L111">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ParallelKMeans.update_containers-Tuple{Hamerly,Any,Any,Any}" href="#ParallelKMeans.update_containers-Tuple{Hamerly,Any,Any,Any}"><code>ParallelKMeans.update_containers</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">update_containers(::Hamerly, containers, centroids, n_threads)</code></pre><p>Calculates minimum distances from centers to each other.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PyDataBlog/ParallelKMeans.jl/blob/a7107daf5ba143382fedf509db2f578cac17f37d/src/hamerly.jl#LL150-L154">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ParallelKMeans.@parallelize-Tuple{Any,Any,Any}" href="#ParallelKMeans.@parallelize-Tuple{Any,Any,Any}"><code>ParallelKMeans.@parallelize</code></a> — <span class="docstring-category">Macro</span></header><section><div><pre><code class="language-julia">@parallelize(n_threads, ncol, f)</code></pre><p>Parallelize function and run it over n_threads. Function should require following conditions:</p><ol><li>It should not return any values.</li><li>It should accept parameters two parameters at the end of the argument list. First</li></ol><p>accepted parameter is <code>range</code>, which defines chunk used in calculations. Second parameter is <code>idx</code> which defines id of the container where results can be stored.</p><p><code>ncol</code> argument defines range 1:ncol which is sliced in <code>n_threads</code> chunks.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PyDataBlog/ParallelKMeans.jl/blob/a7107daf5ba143382fedf509db2f578cac17f37d/src/kmeans.jl#LL43-L53">source</a></section></article></article></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Tuesday 21 April 2020 08:58">Tuesday 21 April 2020</span>. Using Julia version 1.4.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
