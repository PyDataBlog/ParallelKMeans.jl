<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Home · ParallelKMeans.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">ParallelKMeans.jl</span></div><form class="docs-search" action="search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li class="is-active"><a class="tocitem" href>Home</a><ul class="internal"><li><a class="tocitem" href="#Motivation"><span>Motivation</span></a></li><li><a class="tocitem" href="#K-Means-Algorithm-Implementation-Notes"><span>K-Means Algorithm Implementation Notes</span></a></li><li><a class="tocitem" href="#Installation"><span>Installation</span></a></li><li><a class="tocitem" href="#Features"><span>Features</span></a></li><li><a class="tocitem" href="#Pending-Features"><span>Pending Features</span></a></li><li><a class="tocitem" href="#How-To-Use"><span>How To Use</span></a></li><li><a class="tocitem" href="#Benchmarks"><span>Benchmarks</span></a></li><li><a class="tocitem" href="#Release-History"><span>Release History</span></a></li><li><a class="tocitem" href="#Contributing"><span>Contributing</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Home</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Home</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/PyDataBlog/ParallelKMeans.jl/blob/master/docs/src/index.md#L" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="[ParallelKMeans.jl-Package](https://github.com/PyDataBlog/ParallelKMeans.jl)"><a class="docs-heading-anchor" href="#[ParallelKMeans.jl-Package](https://github.com/PyDataBlog/ParallelKMeans.jl)"><a href="https://github.com/PyDataBlog/ParallelKMeans.jl">ParallelKMeans.jl Package</a></a><a id="[ParallelKMeans.jl-Package](https://github.com/PyDataBlog/ParallelKMeans.jl)-1"></a><a class="docs-heading-anchor-permalink" href="#[ParallelKMeans.jl-Package](https://github.com/PyDataBlog/ParallelKMeans.jl)" title="Permalink"></a></h1><h2 id="Motivation"><a class="docs-heading-anchor" href="#Motivation">Motivation</a><a id="Motivation-1"></a><a class="docs-heading-anchor-permalink" href="#Motivation" title="Permalink"></a></h2><p>It&#39;s actually a funny story led to the development of this package. What started off as a personal toy project trying to re-construct the K-Means algorithm in native Julia blew up after a heated discussion on the Julia Discourse forum when I asked for Julia optimization tips. Long story short, Julia community is an amazing one! Andrey offered his help and together, we decided to push the speed limits of Julia with a parallel implementation of the most famous clustering algorithm. The initial results were mind blowing so we have decided to tidy up the implementation and share with the world as a maintained Julia pacakge.</p><p>Say hello to <code>ParallelKMeans</code>!</p><p>This package aims to utilize the speed of Julia and parallelization (both CPU &amp; GPU) to offer an extremely fast implementation of the K-Means clustering algorithm and its variants via a friendly interface for practioners.</p><p>In short, we hope this package will eventually mature as the &quot;one-stop-shop&quot; for everything K-Means on CPUs and GPUs.</p><h2 id="K-Means-Algorithm-Implementation-Notes"><a class="docs-heading-anchor" href="#K-Means-Algorithm-Implementation-Notes">K-Means Algorithm Implementation Notes</a><a id="K-Means-Algorithm-Implementation-Notes-1"></a><a class="docs-heading-anchor-permalink" href="#K-Means-Algorithm-Implementation-Notes" title="Permalink"></a></h2><p>Since Julia is a column major language, the input (design matrix) expected by the package must be in the following format;</p><ul><li>Design matrix X of size n×m, the i-th column of X <code>(X[:, i])</code> is a single data point in n-dimensional space.</li><li>Thus, the rows of the design matrix represent the feature space with the columns representing all the training samples in this feature space.</li></ul><p>One of the pitfalls of K-Means algorithm is that it can fall into a local minima. This implementation inherits this problem like every implementation does. As a result, it is useful in practice to restart it several times to get the correct results.</p><h2 id="Installation"><a class="docs-heading-anchor" href="#Installation">Installation</a><a id="Installation-1"></a><a class="docs-heading-anchor-permalink" href="#Installation" title="Permalink"></a></h2><p>If you are using  Julia in the recommended <a href="https://junolab.org/">Juno IDE</a>, the number of threads is already set to the number of available CPU cores so multithreading enabled out of the box. For other IDEs, multithreading must be exported in your environment before launching the Julia REPL in the command line.</p><p><em>TIP</em>: One needs to navigate or point to the Julia executable file to be able to launch it in the command line. Enable multi threading on Mac/Linux systems via;</p><pre><code class="language-bash">export JULIA_NUM_THREADS=n  # where n is the number of threads/cores</code></pre><p>For Windows systems:</p><pre><code class="language-bash">set JULIA_NUM_THREADS=n  # where n is the number of threads/cores</code></pre><p>You can grab the latest stable version of this package from Julia registries by simply running;</p><p><em>NB:</em> Don&#39;t forget to invoke Julia&#39;s package manager with <code>]</code></p><pre><code class="language-julia">pkg&gt; add ParallelKMeans</code></pre><p>The few (and selected) brave ones can simply grab the current experimental features by simply adding the master branch to your development environment after invoking the package manager with <code>]</code>:</p><pre><code class="language-julia">pkg&gt; add ParallelKMeans#master</code></pre><p>You are good to go with bleeding edge features and breakages!</p><p>To revert to a stable version, you can simply run:</p><pre><code class="language-julia">pkg&gt; free ParallelKMeans</code></pre><h2 id="Features"><a class="docs-heading-anchor" href="#Features">Features</a><a id="Features-1"></a><a class="docs-heading-anchor-permalink" href="#Features" title="Permalink"></a></h2><ul><li>Lightning fast implementation of Kmeans clustering algorithm even on a single thread in native Julia.</li><li>Support for multi-threading implementation of K-Means clustering algorithm.</li><li>&#39;Kmeans++&#39; initialization for faster and better convergence.</li><li>Implementation of available classic and contemporary variants of the K-Means algorithm.</li></ul><h2 id="Pending-Features"><a class="docs-heading-anchor" href="#Pending-Features">Pending Features</a><a id="Pending-Features-1"></a><a class="docs-heading-anchor-permalink" href="#Pending-Features" title="Permalink"></a></h2><ul><li>[X] Implementation of <a href="https://www.researchgate.net/publication/220906984_Making_k-means_Even_Faster">Hamerly implementation</a>.</li><li>[X] Interface for inclusion in Alan Turing Institute&#39;s <a href="https://github.com/alan-turing-institute/MLJModels.jl#who-is-this-repo-for">MLJModels</a>.</li><li>[X] Full Implementation of Triangle inequality based on <a href="https://www.aaai.org/Papers/ICML/2003/ICML03-022.pdf">Elkan - 2003 Using the Triangle Inequality to Accelerate K-Means&quot;</a>.</li><li>[X] Implementation of <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/ding15.pdf">Yinyang K-Means: A Drop-In Replacement of the Classic K-Means with Consistent Speedup</a>.</li><li>[X] Implementation of <a href="http://proceedings.mlr.press/v51/lucic16-supp.pdf">Coresets</a>.</li><li>[X] Support for weighted K-means.</li><li>[X] Support of MLJ Random generation hyperparameter.</li><li>[ ] Support for other distance metrics supported by <a href="https://github.com/JuliaStats/Distances.jl#supported-distances">Distances.jl</a>.</li><li>[ ] Implementation of <a href="http://cs.baylor.edu/~hamerly/papers/sdm2016_rysavy_hamerly.pdf">Geometric methods to accelerate k-means algorithm</a>.</li><li>[ ] Native support for tabular data inputs outside of MLJModels&#39; interface.</li><li>[ ] Refactoring and finalization of API design.</li><li>[ ] GPU support.</li><li>[ ] Distributed calculations support.</li><li>[ ] Optimization of code base.</li><li>[ ] Improved Documentation</li><li>[ ] More benchmark tests.</li></ul><h2 id="How-To-Use"><a class="docs-heading-anchor" href="#How-To-Use">How To Use</a><a id="How-To-Use-1"></a><a class="docs-heading-anchor-permalink" href="#How-To-Use" title="Permalink"></a></h2><p>Taking advantage of Julia&#39;s brilliant multiple dispatch system, the package exposes users to a very easy-to-use API.</p><pre><code class="language-julia">using ParallelKMeans

# Uses all available CPU cores by default
multi_results = kmeans(X, 3; max_iters=300)

# Use only 1 core of CPU
results = kmeans(X, 3; n_threads=1, max_iters=300)</code></pre><p>The main design goal is to offer all available variations of the KMeans algorithm to end users as composable elements. By default, Lloyd&#39;s implementation is used but users can specify different variations of the KMeans clustering algorithm via this interface;</p><pre><code class="language-julia">some_results = kmeans([algo], input_matrix, k; kwargs)

# example
r = kmeans(Lloyd(), X, 3)  # same result as the default</code></pre><pre><code class="language-julia"># r contains all the learned artifacts that can be accessed as;
r.centers               # cluster centers (d x k)
r.assignments           # label assignments (n)
r.totalcost             # total cost (i.e. objective)
r.iterations            # number of elapsed iterations
r.converged             # whether the procedure converged</code></pre><h3 id="Supported-KMeans-algorithm-variations-and-recommended-use-cases"><a class="docs-heading-anchor" href="#Supported-KMeans-algorithm-variations-and-recommended-use-cases">Supported KMeans algorithm variations and recommended use cases</a><a id="Supported-KMeans-algorithm-variations-and-recommended-use-cases-1"></a><a class="docs-heading-anchor-permalink" href="#Supported-KMeans-algorithm-variations-and-recommended-use-cases" title="Permalink"></a></h3><ul><li><a href="https://cs.nyu.edu/~roweis/csc2515-2006/readings/lloyd57.pdf">Lloyd()</a>  - Default algorithm but only recommended for very small matrices (switch to <code>n_threads = 1</code> to avoid overhead).</li><li><a href="https://www.researchgate.net/publication/220906984_Making_k-means_Even_Faster">Hamerly()</a> - Hamerly is good for moderate number of clusters (&lt; 50?) and moderate dimensions (&lt;100?).</li><li><a href="https://www.aaai.org/Papers/ICML/2003/ICML03-022.pdf">Elkan()</a> - Recommended for high dimensional data.</li><li><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/ding15.pdf">Yinyang()</a> - Recommended for large dimensions and/or large number of clusters.</li><li><a href="http://proceedings.mlr.press/v51/lucic16-supp.pdf">Coreset()</a> - Recommended for very fast clustering of very large datasets, when extreme accuracy is not important.</li><li><a href="http://cs.baylor.edu/~hamerly/papers/sdm2016_rysavy_hamerly.pdf">Geometric()</a> - (Coming soon)</li><li><a href="https://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf">MiniBatch()</a> - (Coming soon)</li></ul><h3 id="Practical-Usage-Examples"><a class="docs-heading-anchor" href="#Practical-Usage-Examples">Practical Usage Examples</a><a id="Practical-Usage-Examples-1"></a><a class="docs-heading-anchor-permalink" href="#Practical-Usage-Examples" title="Permalink"></a></h3><p>Some of the common usage examples of this package are as follows:</p><h4 id="Clustering-With-A-Desired-Number-Of-Groups"><a class="docs-heading-anchor" href="#Clustering-With-A-Desired-Number-Of-Groups">Clustering With A Desired Number Of Groups</a><a id="Clustering-With-A-Desired-Number-Of-Groups-1"></a><a class="docs-heading-anchor-permalink" href="#Clustering-With-A-Desired-Number-Of-Groups" title="Permalink"></a></h4><pre><code class="language-julia">using ParallelKMeans, RDatasets, Plots

# load the data
iris = dataset(&quot;datasets&quot;, &quot;iris&quot;);

# features to use for clustering
features = collect(Matrix(iris[:, 1:4])&#39;);

# various artifacts can be accessed from the result i.e. assigned labels, cost value etc
result = kmeans(features, 3);

# plot with the point color mapped to the assigned cluster index
scatter(iris.PetalLength, iris.PetalWidth, marker_z=result.assignments,
        color=:lightrainbow, legend=false)
</code></pre><p><img src="iris_example.jpg" alt="Image description"/></p><h4 id="Elbow-Method-For-The-Selection-Of-optimal-number-of-clusters"><a class="docs-heading-anchor" href="#Elbow-Method-For-The-Selection-Of-optimal-number-of-clusters">Elbow Method For The Selection Of optimal number of clusters</a><a id="Elbow-Method-For-The-Selection-Of-optimal-number-of-clusters-1"></a><a class="docs-heading-anchor-permalink" href="#Elbow-Method-For-The-Selection-Of-optimal-number-of-clusters" title="Permalink"></a></h4><pre><code class="language-julia">using ParallelKMeans

# Single Thread Implementation of Lloyd&#39;s Algorithm
b = [ParallelKMeans.kmeans(X, i, n_threads=1; tol=1e-6, max_iters=300, verbose=false).totalcost for i = 2:10]

# Multi-threaded Implementation of Lloyd&#39;s Algorithm by default
c = [ParallelKMeans.kmeans(X, i; tol=1e-6, max_iters=300, verbose=false).totalcost for i = 2:10]
</code></pre><h2 id="Benchmarks"><a class="docs-heading-anchor" href="#Benchmarks">Benchmarks</a><a id="Benchmarks-1"></a><a class="docs-heading-anchor-permalink" href="#Benchmarks" title="Permalink"></a></h2><p>Currently, this package is benchmarked against similar implementations in both Python and Julia. All reproducible benchmarks can be found in <a href="https://github.com/PyDataBlog/ParallelKMeans.jl/tree/master/extras">ParallelKMeans/extras</a> directory. More tests in various languages are planned beyond the initial release version (<code>0.1.0</code>).</p><p><em>Note</em>: All benchmark tests are made on the same computer to help eliminate any bias.</p><table><tr><th style="text-align: center">PC Name</th><th style="text-align: center">CPU</th><th style="text-align: center">Ram</th></tr><tr><td style="text-align: center">iMac (Retina 5K 27-inch 2019)</td><td style="text-align: center">3 GHz 6-Core Intel Core i5</td><td style="text-align: center">8 GB 2667 MHz DDR4</td></tr></table><p>Currently, the benchmark speed tests are based on the search for optimal number of clusters using the <a href="https://en.wikipedia.org/wiki/Elbow_method_(clustering)">Elbow Method</a> since this is a practical use case for most practioners employing the K-Means algorithm.</p><h3 id="Benchmark-Results"><a class="docs-heading-anchor" href="#Benchmark-Results">Benchmark Results</a><a id="Benchmark-Results-1"></a><a class="docs-heading-anchor-permalink" href="#Benchmark-Results" title="Permalink"></a></h3><p><img src="benchmark_image.png" alt="benchmark_image.png"/></p><p>_________________________________________________________________________________________________________</p><table><tr><th style="text-align: center">1 million sample (secs)</th><th style="text-align: center">100k sample (secs)</th><th style="text-align: center">10k sample (secs)</th><th style="text-align: center">1k sample (secs)</th><th style="text-align: center">package</th><th style="text-align: center">language</th></tr><tr><td style="text-align: center">538.53100</td><td style="text-align: center">33.15700</td><td style="text-align: center">0.74238</td><td style="text-align: center">0.01710</td><td style="text-align: center">Clustering.jl</td><td style="text-align: center">Julia</td></tr><tr><td style="text-align: center">220.35700</td><td style="text-align: center">20.93600</td><td style="text-align: center">0.82430</td><td style="text-align: center">0.02639</td><td style="text-align: center">mlpack</td><td style="text-align: center">C++ Wrapper</td></tr><tr><td style="text-align: center">20.55400</td><td style="text-align: center">2.91300</td><td style="text-align: center">0.17559</td><td style="text-align: center">0.00609</td><td style="text-align: center">Lloyd</td><td style="text-align: center">Julia</td></tr><tr><td style="text-align: center">11.51800</td><td style="text-align: center">0.96637</td><td style="text-align: center">0.09990</td><td style="text-align: center">0.00635</td><td style="text-align: center">Hamerly</td><td style="text-align: center">Julia</td></tr><tr><td style="text-align: center">14.01900</td><td style="text-align: center">1.13100</td><td style="text-align: center">0.07912</td><td style="text-align: center">0.00646</td><td style="text-align: center">Elkan</td><td style="text-align: center">Julia</td></tr><tr><td style="text-align: center">9.97000</td><td style="text-align: center">1.14600</td><td style="text-align: center">0.10834</td><td style="text-align: center">0.00704</td><td style="text-align: center">Yinyang</td><td style="text-align: center">Julia</td></tr><tr><td style="text-align: center">1,430.00000</td><td style="text-align: center">146.00000</td><td style="text-align: center">5.77000</td><td style="text-align: center">0.34400</td><td style="text-align: center">Sklearn KMeans</td><td style="text-align: center">Python</td></tr><tr><td style="text-align: center">30.10000</td><td style="text-align: center">3.75000</td><td style="text-align: center">0.61300</td><td style="text-align: center">0.20100</td><td style="text-align: center">Sklearn MiniBatchKMeans</td><td style="text-align: center">Python</td></tr><tr><td style="text-align: center">218.20000</td><td style="text-align: center">15.51000</td><td style="text-align: center">0.73370</td><td style="text-align: center">0.01947</td><td style="text-align: center">Knor</td><td style="text-align: center">R</td></tr></table><p>_________________________________________________________________________________________________________</p><h2 id="Release-History"><a class="docs-heading-anchor" href="#Release-History">Release History</a><a id="Release-History-1"></a><a class="docs-heading-anchor-permalink" href="#Release-History" title="Permalink"></a></h2><ul><li>0.1.0 Initial release.</li><li>0.1.1 Added interface for MLJ.</li><li>0.1.2 Added <code>Elkan</code> algorithm.</li><li>0.1.3 Faster &amp; optimized execution.</li><li>0.1.4 Bug fixes.</li><li>0.1.5 Added <code>Yinyang</code> algorithm.</li><li>0.1.6 Added support for weighted k-means; Added <code>Coreset</code> algorithm; improved support for different types of the design matrix.</li><li>0.1.7 Added <code>Yinyang</code> and <code>Coreset</code> support in MLJ interface; added <code>weights</code> support in MLJ; added RNG seed support in MLJ interface and through all algorithms; added metric support.</li><li>0.1.8 Minor cleanup</li><li>0.1.9 Added travis support for Julia 1.5</li></ul><h2 id="Contributing"><a class="docs-heading-anchor" href="#Contributing">Contributing</a><a id="Contributing-1"></a><a class="docs-heading-anchor-permalink" href="#Contributing" title="Permalink"></a></h2><p>Ultimately, we see this package as potentially the one-stop-shop for everything related to KMeans algorithm and its speed up variants. We are open to new implementations and ideas from anyone interested in this project.</p><p>Detailed contribution guidelines will be added in upcoming releases.</p><p>&lt;!–- TODO: Contribution Guidelines –-&gt;</p><ul><li><a href="#ParallelKMeans.AbstractKMeansAlg"><code>ParallelKMeans.AbstractKMeansAlg</code></a></li><li><a href="#ParallelKMeans.ClusteringResult"><code>ParallelKMeans.ClusteringResult</code></a></li><li><a href="#ParallelKMeans.Coreset"><code>ParallelKMeans.Coreset</code></a></li><li><a href="#ParallelKMeans.Elkan"><code>ParallelKMeans.Elkan</code></a></li><li><a href="#ParallelKMeans.Hamerly"><code>ParallelKMeans.Hamerly</code></a></li><li><a href="#ParallelKMeans.KmeansResult"><code>ParallelKMeans.KmeansResult</code></a></li><li><a href="#ParallelKMeans.Lloyd"><code>ParallelKMeans.Lloyd</code></a></li><li><a href="#ParallelKMeans.Yinyang"><code>ParallelKMeans.Yinyang</code></a></li><li><a href="#MLJModelInterface.fit-Tuple{ParallelKMeans.KMeans,Int64,Any}"><code>MLJModelInterface.fit</code></a></li><li><a href="#ParallelKMeans.chunk_colwise-NTuple{8,Any}"><code>ParallelKMeans.chunk_colwise</code></a></li><li><a href="#ParallelKMeans.chunk_initialize-Tuple{Hamerly,Any,Any,Any,Any,Any,Any,Any}"><code>ParallelKMeans.chunk_initialize</code></a></li><li><a href="#ParallelKMeans.chunk_update_bounds-Tuple{Hamerly,Any,Any,Any,Any,Any,Distances.Euclidean,Any,Any}"><code>ParallelKMeans.chunk_update_bounds</code></a></li><li><a href="#ParallelKMeans.chunk_update_bounds-Tuple{Hamerly,Any,Any,Any,Any,Any,Distances.Metric,Any,Any}"><code>ParallelKMeans.chunk_update_bounds</code></a></li><li><a href="#ParallelKMeans.chunk_update_centroids-Tuple{Hamerly,Any,Any,Any,Any,Any,Any,Any}"><code>ParallelKMeans.chunk_update_centroids</code></a></li><li><a href="#ParallelKMeans.create_containers-Tuple{Lloyd,Any,Any,Any,Any,Any}"><code>ParallelKMeans.create_containers</code></a></li><li><a href="#ParallelKMeans.distance-Tuple{Distances.Euclidean,Any,Any,Any,Any}"><code>ParallelKMeans.distance</code></a></li><li><a href="#ParallelKMeans.distance-NTuple{5,Any}"><code>ParallelKMeans.distance</code></a></li><li><a href="#ParallelKMeans.double_argmax-Union{Tuple{AbstractArray{T,1}}, Tuple{T}} where T"><code>ParallelKMeans.double_argmax</code></a></li><li><a href="#ParallelKMeans.kmeans-Tuple{ParallelKMeans.AbstractKMeansAlg,Any,Any}"><code>ParallelKMeans.kmeans</code></a></li><li><a href="#ParallelKMeans.kmeans!"><code>ParallelKMeans.kmeans!</code></a></li><li><a href="#ParallelKMeans.move_centers-Tuple{Hamerly,Any,Any,Any}"><code>ParallelKMeans.move_centers</code></a></li><li><a href="#ParallelKMeans.point_all_centers!-NTuple{5,Any}"><code>ParallelKMeans.point_all_centers!</code></a></li><li><a href="#ParallelKMeans.point_all_centers!-Tuple{Yinyang,Any,Any,Any,Any,Any}"><code>ParallelKMeans.point_all_centers!</code></a></li><li><a href="#ParallelKMeans.smart_init"><code>ParallelKMeans.smart_init</code></a></li><li><a href="#ParallelKMeans.splitter-Tuple{Any,Any}"><code>ParallelKMeans.splitter</code></a></li><li><a href="#ParallelKMeans.sum_of_squares-NTuple{8,Any}"><code>ParallelKMeans.sum_of_squares</code></a></li><li><a href="#ParallelKMeans.update_containers-Tuple{Hamerly,Any,Any,Any,Any}"><code>ParallelKMeans.update_containers</code></a></li><li><a href="#ParallelKMeans.@parallelize-Tuple{Any,Any,Any}"><code>ParallelKMeans.@parallelize</code></a></li></ul><article class="docstring"><header><a class="docstring-binding" id="ParallelKMeans.AbstractKMeansAlg" href="#ParallelKMeans.AbstractKMeansAlg"><code>ParallelKMeans.AbstractKMeansAlg</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">AbstractKMeansAlg</code></pre><p>Abstract base type inherited by all sub-KMeans algorithms.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PyDataBlog/ParallelKMeans.jl/blob/1d5d0c609d79084cb206e945c728a8b931a421ff/src/kmeans.jl#LL2-L6">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ParallelKMeans.ClusteringResult" href="#ParallelKMeans.ClusteringResult"><code>ParallelKMeans.ClusteringResult</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">ClusteringResult</code></pre><p>Base type for the output of clustering algorithm.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PyDataBlog/ParallelKMeans.jl/blob/1d5d0c609d79084cb206e945c728a8b931a421ff/src/kmeans.jl#LL10-L14">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ParallelKMeans.Coreset" href="#ParallelKMeans.Coreset"><code>ParallelKMeans.Coreset</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Coreset()</code></pre><p>Coreset algorithm implementation, based on &quot;Lucic, Mario &amp; Bachem, Olivier &amp; Krause, Andreas. (2015). Strong Coresets for Hard and Soft Bregman Clustering with Applications to Exponential Family Mixtures.&quot;</p><p><code>Coreset</code> supports following arguments:</p><ul><li><code>m</code>: default 100, subsample size</li><li><code>alg</code>: default <code>Lloyd()</code>, algorithm used to clusterize sample</li></ul><p>It can be used directly in <code>kmeans</code> function</p><pre><code class="language-julia">X = rand(30, 100_000)   # 100_000 random points in 30 dimensions

# 3 clusters, Coreset algorithm with default Lloyd algorithm and 100 subsamples
kmeans(Coreset(), X, 3)

# 3 clusters, Coreset algorithm with Hamerly algorithm and 500 subsamples
kmeans(Coreset(m = 500, alg = Hamerly()), X, 3)
kmeans(Coreset(500, Hamerly()), X, 3)

# alternatively short form can be used for defining subsample size or algorithm only
kmeans(Coreset(500), X, 3) # sample of the size 500, Lloyd clustering algorithm
kmeans(Coreset(Hamerly()), X, 3) # sample of the size 100, Hamerly clustering algorithm</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PyDataBlog/ParallelKMeans.jl/blob/1d5d0c609d79084cb206e945c728a8b931a421ff/src/coreset.jl#LL1-L28">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ParallelKMeans.Elkan" href="#ParallelKMeans.Elkan"><code>ParallelKMeans.Elkan</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Elkan()</code></pre><p>Elkan algorithm implementation, based on &quot;Charles Elkan. 2003. Using the triangle inequality to accelerate k-means. In Proceedings of the Twentieth International Conference on International Conference on Machine Learning (ICML’03). AAAI Press, 147–153.&quot;</p><p>This algorithm provides much faster convergence than Lloyd algorithm especially for high dimensional data. It can be used directly in <code>kmeans</code> function</p><pre><code class="language-julia">X = rand(30, 100_000)   # 100_000 random points in 30 dimensions

kmeans(Elkan(), X, 3) # 3 clusters, Elkan algorithm</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PyDataBlog/ParallelKMeans.jl/blob/1d5d0c609d79084cb206e945c728a8b931a421ff/src/elkan.jl#LL1-L18">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ParallelKMeans.Hamerly" href="#ParallelKMeans.Hamerly"><code>ParallelKMeans.Hamerly</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Hamerly()</code></pre><p>Hamerly algorithm implementation, based on &quot;Hamerly, Greg. (2010). Making k-means Even Faster.  Proceedings of the 2010 SIAM International Conference on Data Mining. 130-140. 10.1137/1.9781611972801.12.&quot;</p><p>This algorithm provides much faster convergence than Lloyd algorithm with realtively small increase in memory footprint. It is especially suitable for low to medium dimensional input data.</p><p>It can be used directly in <code>kmeans</code> function</p><pre><code class="language-julia">X = rand(30, 100_000)   # 100_000 random points in 30 dimensions

kmeans(Hamerly(), X, 3) # 3 clusters, Hamerly algorithm</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PyDataBlog/ParallelKMeans.jl/blob/1d5d0c609d79084cb206e945c728a8b931a421ff/src/hamerly.jl#LL1-L17">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ParallelKMeans.KmeansResult" href="#ParallelKMeans.KmeansResult"><code>ParallelKMeans.KmeansResult</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">KmeansResult{C,D&lt;:Real,WC&lt;:Real} &lt;: ClusteringResult</code></pre><p>The output of <a href="#ParallelKMeans.kmeans-Tuple{ParallelKMeans.AbstractKMeansAlg,Any,Any}"><code>kmeans</code></a> and <a href="#ParallelKMeans.kmeans!"><code>kmeans!</code></a>.</p><p><strong>Type parameters</strong></p><ul><li><code>C&lt;:AbstractMatrix{&lt;:AbstractFloat}</code>: type of the <code>centers</code> matrix</li><li><code>D&lt;:Real</code>: type of the assignment cost</li><li><code>WC&lt;:Real</code>: type of the cluster weight</li></ul><p><strong>C is the type of centers, an (abstract) matrix of size (d x k)</strong></p><p><strong>D is the type of pairwise distance computation from points to cluster centers</strong></p><p><strong>WC is the type of cluster weights, either Int (in the case where points are</strong></p><p><strong>unweighted) or eltype(weights) (in the case where points are weighted).</strong></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PyDataBlog/ParallelKMeans.jl/blob/1d5d0c609d79084cb206e945c728a8b931a421ff/src/kmeans.jl#LL19-L31">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ParallelKMeans.Lloyd" href="#ParallelKMeans.Lloyd"><code>ParallelKMeans.Lloyd</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Lloyd &lt;: AbstractKMeansAlg</code></pre><p>Basic algorithm for k-means calculation.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PyDataBlog/ParallelKMeans.jl/blob/1d5d0c609d79084cb206e945c728a8b931a421ff/src/lloyd.jl#LL1-L5">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ParallelKMeans.Yinyang" href="#ParallelKMeans.Yinyang"><code>ParallelKMeans.Yinyang</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Yinyang()</code></pre><p>Yinyang algorithm implementation, based on &quot;Yufei Ding et al. 2015. Yinyang K-Means: A Drop-In Replacement of the Classic K-Means with Consistent Speedup. Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015&quot;</p><p>Generally it outperform <code>Hamerly</code> algorithm and has roughly the same time as <code>Elkan</code> algorithm with much lower memory consumption.</p><p><code>Yinyang</code> supports following arguments: <code>auto</code>: <code>Bool</code>, indicates whether to perform automated or manual grouping <code>group_size</code>: <code>Int</code>, estimation of average number of clusters per group. Lower numbers corresponds to higher calculation speed and higher memory consumption and vice versa.</p><p>It can be used directly in <code>kmeans</code> function</p><pre><code class="language-julia">X = rand(30, 100_000)   # 100_000 random points in 30 dimensions

# 3 clusters, Yinyang algorithm, with deault 7 group_size
kmeans(Yinyang(), X, 3)

# Following are equivalent
# 3 clusters, Yinyang algorithm with 10 group_size
kmeans(Yinyang(group_size = 10), X, 3)
kmeans(Yinyang(10), X, 3)

# One group with the size of the number of points
kmeans(Yinyang(auto = false), X, 3)
kmeans(Yinyang(false), X, 3)

# Chinese writing can be used
kmeans(阴阳(), X, 3)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PyDataBlog/ParallelKMeans.jl/blob/1d5d0c609d79084cb206e945c728a8b931a421ff/src/yinyang.jl#LL1-L37">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJModelInterface.fit-Tuple{ParallelKMeans.KMeans,Int64,Any}" href="#MLJModelInterface.fit-Tuple{ParallelKMeans.KMeans,Int64,Any}"><code>MLJModelInterface.fit</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">Fit the specified ParaKMeans model constructed by the user.

See also the [package documentation](https://pydatablog.github.io/ParallelKMeans.jl/stable).</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PyDataBlog/ParallelKMeans.jl/blob/1d5d0c609d79084cb206e945c728a8b931a421ff/src/mlj_interface.jl#LL82-L86">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ParallelKMeans.chunk_colwise-NTuple{8,Any}" href="#ParallelKMeans.chunk_colwise-NTuple{8,Any}"><code>ParallelKMeans.chunk_colwise</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">chunk_colwise!(target, x, y, i, weights, r, idx)</code></pre><p>Utility function for the calculation of the weighted distance between points <code>x</code> and centroid vector <code>y[:, i]</code>. UnitRange argument <code>r</code> select subarray of original design matrix <code>x</code> that is going to be processed.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PyDataBlog/ParallelKMeans.jl/blob/1d5d0c609d79084cb206e945c728a8b931a421ff/src/seeding.jl#LL1-L8">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ParallelKMeans.chunk_initialize-Tuple{Hamerly,Any,Any,Any,Any,Any,Any,Any}" href="#ParallelKMeans.chunk_initialize-Tuple{Hamerly,Any,Any,Any,Any,Any,Any,Any}"><code>ParallelKMeans.chunk_initialize</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">chunk_initialize(alg::Hamerly, containers, centroids, X, weights, metric, r, idx)</code></pre><p>Initial calulation of all bounds and points labeling.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PyDataBlog/ParallelKMeans.jl/blob/1d5d0c609d79084cb206e945c728a8b931a421ff/src/hamerly.jl#LL121-L125">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ParallelKMeans.chunk_update_bounds-Tuple{Hamerly,Any,Any,Any,Any,Any,Distances.Euclidean,Any,Any}" href="#ParallelKMeans.chunk_update_bounds-Tuple{Hamerly,Any,Any,Any,Any,Any,Distances.Euclidean,Any,Any}"><code>ParallelKMeans.chunk_update_bounds</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">chunk_update_bounds(alg::Hamerly, containers, r1, r2, pr1, pr2, metric::Euclidean, r, idx)</code></pre><p>Updates upper and lower bounds of point distance to the centers, with regard to the centers movement when metric is Euclidean.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PyDataBlog/ParallelKMeans.jl/blob/1d5d0c609d79084cb206e945c728a8b931a421ff/src/hamerly.jl#LL258-L263">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ParallelKMeans.chunk_update_bounds-Tuple{Hamerly,Any,Any,Any,Any,Any,Distances.Metric,Any,Any}" href="#ParallelKMeans.chunk_update_bounds-Tuple{Hamerly,Any,Any,Any,Any,Any,Distances.Metric,Any,Any}"><code>ParallelKMeans.chunk_update_bounds</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">chunk_update_bounds(alg::Hamerly, containers, r1, r2, pr1, pr2, metric::Metric, r, idx)</code></pre><p>Updates upper and lower bounds of point distance to the centers, with regard to the centers movement when metric is Euclidean.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PyDataBlog/ParallelKMeans.jl/blob/1d5d0c609d79084cb206e945c728a8b931a421ff/src/hamerly.jl#LL299-L304">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ParallelKMeans.chunk_update_centroids-Tuple{Hamerly,Any,Any,Any,Any,Any,Any,Any}" href="#ParallelKMeans.chunk_update_centroids-Tuple{Hamerly,Any,Any,Any,Any,Any,Any,Any}"><code>ParallelKMeans.chunk_update_centroids</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">chunk_update_centroids(alg::Hamerly, containers, centroids, X, weights, metric, r, idx)</code></pre><p>Detailed description of this function can be found in the original paper. It iterates through all points and tries to skip some calculation using known upper and lower bounds of distances from point to centers. If it fails to skip than it fall back to generic <code>point_all_centers!</code> function.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PyDataBlog/ParallelKMeans.jl/blob/1d5d0c609d79084cb206e945c728a8b931a421ff/src/hamerly.jl#LL160-L166">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ParallelKMeans.create_containers-Tuple{Lloyd,Any,Any,Any,Any,Any}" href="#ParallelKMeans.create_containers-Tuple{Lloyd,Any,Any,Any,Any,Any}"><code>ParallelKMeans.create_containers</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">create_containers(::Lloyd, k, nrow, ncol, n_threads)</code></pre><p>Internal function for the creation of all necessary intermidiate structures.</p><ul><li><code>centroids_new</code> - container which holds new positions of centroids</li><li><code>centroids_cnt</code> - container which holds number of points for each centroid</li><li><code>labels</code> - vector which holds labels of corresponding points</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PyDataBlog/ParallelKMeans.jl/blob/1d5d0c609d79084cb206e945c728a8b931a421ff/src/lloyd.jl#LL77-L85">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ParallelKMeans.distance-NTuple{5,Any}" href="#ParallelKMeans.distance-NTuple{5,Any}"><code>ParallelKMeans.distance</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">distance(metric, X1, X2, i1, i2)</code></pre><p>Allocationless calculation of distance between vectors X1[:, i1] and X2[:, i2] defined by the supplied distance metric.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PyDataBlog/ParallelKMeans.jl/blob/1d5d0c609d79084cb206e945c728a8b931a421ff/src/kmeans.jl#LL102-L106">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ParallelKMeans.distance-Tuple{Distances.Euclidean,Any,Any,Any,Any}" href="#ParallelKMeans.distance-Tuple{Distances.Euclidean,Any,Any,Any,Any}"><code>ParallelKMeans.distance</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">distance(X1, X2, i1, i2)</code></pre><p>Allocationless calculation of square eucledean distance between vectors X1[:, i1] and X2[:, i2]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PyDataBlog/ParallelKMeans.jl/blob/1d5d0c609d79084cb206e945c728a8b931a421ff/src/kmeans.jl#LL110-L114">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ParallelKMeans.double_argmax-Union{Tuple{AbstractArray{T,1}}, Tuple{T}} where T" href="#ParallelKMeans.double_argmax-Union{Tuple{AbstractArray{T,1}}, Tuple{T}} where T"><code>ParallelKMeans.double_argmax</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">double_argmax(p)</code></pre><p>Finds maximum and next after maximum arguments.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PyDataBlog/ParallelKMeans.jl/blob/1d5d0c609d79084cb206e945c728a8b931a421ff/src/hamerly.jl#LL321-L325">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ParallelKMeans.kmeans!" href="#ParallelKMeans.kmeans!"><code>ParallelKMeans.kmeans!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Kmeans!(alg::AbstractKMeansAlg, containers, design_matrix, k; n_threads = nthreads(), k_init=&quot;k-means++&quot;, max_iters=300, tol=1e-6, verbose=true)</code></pre><p>Mutable version of <code>kmeans</code> function. Definition of arguments and results can be found in <code>kmeans</code>.</p><p>Argument <code>containers</code> represent algorithm specific containers, such as labels, intermidiate centroids and so on, which are used during calculations.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PyDataBlog/ParallelKMeans.jl/blob/1d5d0c609d79084cb206e945c728a8b931a421ff/src/lloyd.jl#LL8-L16">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ParallelKMeans.kmeans-Tuple{ParallelKMeans.AbstractKMeansAlg,Any,Any}" href="#ParallelKMeans.kmeans-Tuple{ParallelKMeans.AbstractKMeansAlg,Any,Any}"><code>ParallelKMeans.kmeans</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">kmeans([alg::AbstractKMeansAlg,] design_matrix, k; n_threads = nthreads(),
k_init=&quot;k-means++&quot;, max_iters=300, tol=1e-6, verbose=true, rng = Random.GLOBAL_RNG)</code></pre><p>This main function employs the K-means algorithm to cluster all examples in the training data (design_matrix) into k groups using either the <code>k-means++</code> or random initialisation technique for selecting the initial centroids.</p><p>At the end of the number of iterations specified (max_iters), convergence is achieved if difference between the current and last cost objective is less than the tolerance level (tol). An error is thrown if convergence fails.</p><p>Arguments:</p><ul><li><code>alg</code> defines one of the algorithms used to calculate <code>k-means</code>. This</li></ul><p>argument can be omitted, by default Lloyd algorithm is used.</p><ul><li><code>n_threads</code> defines number of threads used for calculations, by default it is equal</li></ul><p>to the <code>Threads.nthreads()</code> which is defined by <code>JULIA_NUM_THREADS</code> environmental variable. For small size design matrices it make sense to set this argument to 1 in order to avoid overhead of threads generation.</p><ul><li><code>k_init</code> is one of the algorithms used for initialization. By default <code>k-means++</code> algorithm is used,</li></ul><p>alternatively one can use <code>rand</code> to choose random points for init.</p><ul><li><code>max_iters</code> is the maximum number of iterations</li><li><code>tol</code> defines tolerance for early stopping.</li><li><code>verbose</code> is verbosity level. Details of operations can be either printed or not by setting verbose accordingly.</li></ul><p>A <code>KmeansResult</code> structure representing labels, centroids, and sum_squares is returned.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PyDataBlog/ParallelKMeans.jl/blob/1d5d0c609d79084cb206e945c728a8b931a421ff/src/kmeans.jl#LL145-L172">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ParallelKMeans.move_centers-Tuple{Hamerly,Any,Any,Any}" href="#ParallelKMeans.move_centers-Tuple{Hamerly,Any,Any,Any}"><code>ParallelKMeans.move_centers</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">move_centers(::Hamerly, containers, centroids, metric)</code></pre><p>Calculates new positions of centers and distance they have moved. Results are stored in <code>centroids</code> and <code>p</code> respectively.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PyDataBlog/ParallelKMeans.jl/blob/1d5d0c609d79084cb206e945c728a8b931a421ff/src/hamerly.jl#LL237-L242">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ParallelKMeans.point_all_centers!-NTuple{5,Any}" href="#ParallelKMeans.point_all_centers!-NTuple{5,Any}"><code>ParallelKMeans.point_all_centers!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">point_all_centers!(containers, centroids, X, i, metric)</code></pre><p>Calculates new labels and upper and lower bounds for all points.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PyDataBlog/ParallelKMeans.jl/blob/1d5d0c609d79084cb206e945c728a8b931a421ff/src/hamerly.jl#LL204-L208">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ParallelKMeans.point_all_centers!-Tuple{Yinyang,Any,Any,Any,Any,Any}" href="#ParallelKMeans.point_all_centers!-Tuple{Yinyang,Any,Any,Any,Any,Any}"><code>ParallelKMeans.point_all_centers!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">point_all_centers!(containers, centroids, X, i)</code></pre><p>Calculates new labels and upper and lower bounds for all points.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PyDataBlog/ParallelKMeans.jl/blob/1d5d0c609d79084cb206e945c728a8b931a421ff/src/yinyang.jl#LL373-L377">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ParallelKMeans.smart_init" href="#ParallelKMeans.smart_init"><code>ParallelKMeans.smart_init</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">smart_init(X, k; init=&quot;k-means++&quot;)</code></pre><p>This function handles the random initialisation of the centroids from the design matrix (X) and desired groups (k) that a user supplies.</p><p><code>k-means++</code> algorithm is used by default with the normal random selection of centroids from X used if any other string is attempted.</p><p>A named tuple representing centroids and indices respecitively is returned.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PyDataBlog/ParallelKMeans.jl/blob/1d5d0c609d79084cb206e945c728a8b931a421ff/src/seeding.jl#LL19-L29">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ParallelKMeans.splitter-Tuple{Any,Any}" href="#ParallelKMeans.splitter-Tuple{Any,Any}"><code>ParallelKMeans.splitter</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">spliiter(n, k)</code></pre><p>Internal utility function, splits 1:n sequence to k chunks of approximately same size.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PyDataBlog/ParallelKMeans.jl/blob/1d5d0c609d79084cb206e945c728a8b931a421ff/src/kmeans.jl#LL44-L48">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ParallelKMeans.sum_of_squares-NTuple{8,Any}" href="#ParallelKMeans.sum_of_squares-NTuple{8,Any}"><code>ParallelKMeans.sum_of_squares</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">sum_of_squares(x, labels, centre, k)</code></pre><p>This function computes the total sum of squares based on the assigned (labels) design matrix(x), centroids (centre), and the number of desired groups (k).</p><p>A Float type representing the computed metric is returned.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PyDataBlog/ParallelKMeans.jl/blob/1d5d0c609d79084cb206e945c728a8b931a421ff/src/kmeans.jl#LL127-L134">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ParallelKMeans.update_containers-Tuple{Hamerly,Any,Any,Any,Any}" href="#ParallelKMeans.update_containers-Tuple{Hamerly,Any,Any,Any,Any}"><code>ParallelKMeans.update_containers</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">update_containers(::Hamerly, containers, centroids, n_threads, metric)</code></pre><p>Calculates minimum distances from centers to each other.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PyDataBlog/ParallelKMeans.jl/blob/1d5d0c609d79084cb206e945c728a8b931a421ff/src/hamerly.jl#LL141-L145">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ParallelKMeans.@parallelize-Tuple{Any,Any,Any}" href="#ParallelKMeans.@parallelize-Tuple{Any,Any,Any}"><code>ParallelKMeans.@parallelize</code></a> — <span class="docstring-category">Macro</span></header><section><div><pre><code class="language-julia">@parallelize(n_threads, ncol, f)</code></pre><p>Parallelize function and run it over n_threads. Function should require following conditions:</p><ol><li>It should not return any values.</li><li>It should accept parameters two parameters at the end of the argument list. First</li></ol><p>accepted parameter is <code>range</code>, which defines chunk used in calculations. Second parameter is <code>idx</code> which defines id of the container where results can be stored.</p><p><code>ncol</code> argument defines range 1:ncol which is sliced in <code>n_threads</code> chunks.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/PyDataBlog/ParallelKMeans.jl/blob/1d5d0c609d79084cb206e945c728a8b931a421ff/src/kmeans.jl#LL55-L65">source</a></section></article></article><nav class="docs-footer"><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Thursday 11 February 2021 03:11">Thursday 11 February 2021</span>. Using Julia version 1.5.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
